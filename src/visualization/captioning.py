# src/visualization/captioning.py

"""
Caption visualization utilities
"""

import matplotlib.pyplot as plt
import numpy as np
import torch
from typing import List, Optional, Dict
import os

from ..preprocessing.transforms import denormalize_image
from ..preprocessing.vocabulary import Vocabulary

# Standardized settings for all visualizations
STANDARD_DPI = 300
STANDARD_FIGSIZE = (10, 10)  # Square format for consistency
TRAINING_FIGSIZE = (16, 8)   # Wide format for training curves
FONT_TITLE = 16
FONT_LABEL = 14
FONT_TEXT = 12

def visualize_sample_captions(model: torch.nn.Module, dataset, vocab: Vocabulary, 
                            device: torch.device, num_samples: int = 3, 
                            save_dir: Optional[str] = None):
    """
    Visualize sample captions generated by the model
    
    Args:
        model: Trained model
        dataset: Dataset to sample from
        vocab: Vocabulary object
        device: Device to run on
        num_samples: Number of samples to visualize
        save_dir: Optional directory to save visualizations
    """
    # Determine if this is an attention model
    is_attention_model = hasattr(model, 'caption_image_with_attention')
    
    # Set model to evaluation mode
    model.eval()
    
    # Select random samples
    indices = np.random.choice(len(dataset), num_samples, replace=False)
    
    # Generate captions for each sample
    with torch.no_grad():
        for i, idx in enumerate(indices):
            # Get image and caption
            image, caption = dataset[idx]
            img_name = dataset.data_df.iloc[idx]['image']  # Get image name
            
            # Move to device
            image = image.unsqueeze(0).to(device)
            
            # Generate caption
            if is_attention_model:
                generated_caption, attention_weights = model.caption_image_with_attention(image, vocab)
            else:
                generated_caption = model.caption_image(image, vocab)
                attention_weights = None
            
            # Convert reference caption to words
            reference_caption = []
            for token_idx in caption:
                token = vocab.itos[token_idx.item()]
                if token == "<EOS>":
                    break
                if token not in ["<PAD>", "<SOS>"]:
                    reference_caption.append(token)
            reference_caption = ' '.join(reference_caption)
            
            # Print image name and captions
            print()
            print(f"Image {i+1}: {img_name}")
            print(f"Reference: {reference_caption}")
            print(f"Generated: {generated_caption}")
            
            # Display the image with standardized settings
            fig, ax = plt.subplots(figsize=STANDARD_FIGSIZE)
            
            img = denormalize_image(image[0])
            ax.imshow(img)
            
            # Add captions as text below the image
            caption_text = f"Reference: {reference_caption}\nGenerated: {generated_caption}"
            fig.text(0.5, 0.02, caption_text, ha='center', fontsize=FONT_TEXT, 
                    wrap=True, bbox=dict(boxstyle="round,pad=0.5", facecolor='wheat', alpha=0.7))
            
            ax.set_title(f"Sample {i+1}: {img_name}", fontsize=FONT_TITLE, pad=20)
            ax.axis('off')
            
            plt.tight_layout()
            
            if save_dir:
                save_path = os.path.join(save_dir, f'caption_sample_{i+1}.png')
                plt.savefig(save_path, dpi=STANDARD_DPI, bbox_inches='tight', facecolor='white')
            
            plt.show()
            
            # For attention model, visualize attention weights
            if is_attention_model and attention_weights:
                from .attention import visualize_attention
                if save_dir:
                    att_save_path = os.path.join(save_dir, f'attention_sample_{i+1}.png')
                else:
                    att_save_path = None
                visualize_attention(img, generated_caption.split(), attention_weights, 
                                  save_path=att_save_path)

def plot_training_history(history: Dict, model_name: str = "Model", 
                         save_path: Optional[str] = None):
    """
    Plot training history with proper handling of evaluation frequencies
    
    Args:
        history: Training history dictionary
        model_name: Name of the model
        save_path: Optional path to save the figure
    """
    # Create a figure with standardized size
    plt.figure(figsize=TRAINING_FIGSIZE)
    
    # Plot loss curves
    plt.subplot(1, 2, 1)
    
    # Plot training loss (always available)
    plt.plot(history['epochs'], history['train_losses'], 'o-', label='Train Loss')
    
    # Plot validation loss if available
    if 'val_epochs' in history and history['val_losses']:
        plt.plot(history['val_epochs'], history['val_losses'], 'o-', label='Val Loss')
    
    plt.xlabel('Epoch', fontsize=FONT_LABEL)
    plt.ylabel('Loss', fontsize=FONT_LABEL)
    plt.title(f'{model_name} - Training and Validation Loss', fontsize=FONT_TITLE)
    plt.legend(fontsize=FONT_TEXT)
    plt.grid(True, alpha=0.3)
    
    # Plot BLEU scores if available
    if 'bleu_epochs' in history and history['bleu_scores']:
        plt.subplot(1, 2, 2)
        
        # Extract BLEU scores
        bleu_epochs = history['bleu_epochs']
        bleu1 = [b['bleu1'] for b in history['bleu_scores']]
        bleu2 = [b['bleu2'] for b in history['bleu_scores']]
        bleu3 = [b['bleu3'] for b in history['bleu_scores']]
        bleu4 = [b['bleu4'] for b in history['bleu_scores']]
        
        # Plot BLEU scores
        plt.plot(bleu_epochs, bleu1, 'o-', label='BLEU-1')
        plt.plot(bleu_epochs, bleu2, 'o-', label='BLEU-2')
        plt.plot(bleu_epochs, bleu3, 'o-', label='BLEU-3')
        plt.plot(bleu_epochs, bleu4, 'o-', label='BLEU-4')
        plt.xlabel('Epoch', fontsize=FONT_LABEL)
        plt.ylabel('BLEU Score', fontsize=FONT_LABEL)
        plt.title(f'{model_name} - BLEU Scores', fontsize=FONT_TITLE)
        plt.legend(fontsize=FONT_TEXT)
        plt.grid(True, alpha=0.3)
    
    # Show the figure
    plt.tight_layout()
    
    # Save the figure with standardized settings
    if save_path:
        plt.savefig(save_path, dpi=STANDARD_DPI, bbox_inches='tight', facecolor='white')
    
    plt.show()

def compare_models(baseline_model: torch.nn.Module, attention_model: torch.nn.Module, 
                  dataset, vocab: Vocabulary, device: torch.device, 
                  num_samples: int = 3, save_dir: Optional[str] = None):
    """
    Compare captions generated by baseline and attention models
    
    Args:
        baseline_model: Baseline model
        attention_model: Attention model
        dataset: Dataset to sample from
        vocab: Vocabulary object
        device: Device to run on
        num_samples: Number of samples to compare
        save_dir: Optional directory to save comparisons
    """
    # Set models to evaluation mode
    baseline_model.eval()
    attention_model.eval()
    
    # Select random samples
    indices = np.random.choice(len(dataset), num_samples, replace=False)
    
    # Generate captions for each sample
    with torch.no_grad():
        for i, idx in enumerate(indices):
            # Get image and ground truth caption
            image, caption = dataset[idx]
            img_name = dataset.data_df.iloc[idx]['image']
            
            # Move to device
            image = image.unsqueeze(0).to(device)
            
            # Generate captions
            baseline_caption = baseline_model.caption_image(image, vocab)
            attention_caption, attention_weights = attention_model.caption_image_with_attention(image, vocab)
            
            # Convert ground truth caption to words
            gt_words = [vocab.itos[idx.item()] for idx in caption
                      if idx.item() < len(vocab) and vocab.itos[idx.item()] not in ["<PAD>", "<SOS>", "<EOS>"]]
            gt_caption = ' '.join(gt_words)
            
            # Print image name and captions
            print(f"\nImage {i+1}: {img_name}")
            print(f"Ground Truth: {gt_caption}")
            print(f"Baseline: {baseline_caption}")
            print(f"Attention: {attention_caption}")
            
            # Display image and captions with standardized settings
            plt.figure(figsize=STANDARD_FIGSIZE)
            
            # Display image
            plt.subplot(2, 1, 1)
            img = denormalize_image(image[0])
            plt.imshow(img)
            plt.title(f"Image: {img_name}", fontsize=FONT_TITLE)
            plt.axis('off')
            
            # Display captions comparison
            plt.subplot(2, 1, 2)
            plt.axis('off')
            comparison_text = (
                f"Ground Truth: {gt_caption}\n\n"
                f"Baseline: {baseline_caption}\n\n"
                f"Attention: {attention_caption}"
            )
            plt.text(0.5, 0.5, comparison_text, ha='center', va='center', fontsize=FONT_TEXT, 
                    wrap=True, bbox=dict(boxstyle="round,pad=0.5", facecolor='lightgray', alpha=0.7))
            
            plt.tight_layout()
            
            if save_dir:
                save_path = os.path.join(save_dir, f'model_comparison_{i+1}.png')
                plt.savefig(save_path, dpi=STANDARD_DPI, bbox_inches='tight', facecolor='white')
            
            plt.show()
            
            # Display attention visualization
            attention_words = attention_caption.split()
            if attention_weights and attention_words:
                from .attention import visualize_attention
                # Limit number of words to display for readability
                show_every = max(1, len(attention_words) // 10)
                
                if save_dir:
                    att_save_path = os.path.join(save_dir, f'model_comparison_attention_{i+1}.png')
                else:
                    att_save_path = None
                    
                visualize_attention(
                    img, attention_words, attention_weights,
                    show_every=show_every,
                    save_path=att_save_path
                )