{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Captioning: Exploratory Data Analysis\n",
    "\n",
    "This notebook performs exploratory data analysis on the Flickr8k dataset:\n",
    "- Dataset statistics\n",
    "- Caption analysis\n",
    "- Image properties\n",
    "- Vocabulary analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and imports\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Add project root to path\n",
    "project_root = os.path.dirname(os.getcwd())\n",
    "sys.path.append(project_root)\n",
    "\n",
    "# Import project modules\n",
    "from src.utils.manager import ConfigManager\n",
    "from src.utils.config import load_config\n",
    "from src.utils.constants import SEED\n",
    "from src.preprocessing.vocabulary import preprocess_caption\n",
    "\n",
    "# Set style\n",
    "plt.style.use('default')\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration using ConfigManager\n",
    "from src.utils.manager import ConfigManager\n",
    "\n",
    "# Initialize configuration manager\n",
    "config_manager = ConfigManager()\n",
    "\n",
    "# Get configurations\n",
    "data_config = config_manager.get_data_params()\n",
    "\n",
    "# Get paths from config manager\n",
    "captions_file = data_config['dataset']['captions_file']\n",
    "images_dir = data_config['dataset']['images_dir']\n",
    "\n",
    "print(f\"Captions file: {captions_file}\")\n",
    "print(f\"Images directory: {images_dir}\")\n",
    "\n",
    "# Check if files exist\n",
    "print(f\"\\nCaptions file exists: {os.path.exists(captions_file)}\")\n",
    "print(f\"Images directory exists: {os.path.exists(images_dir)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Explore Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: captions_file path is already set from the previous cell\n",
    "print(f\"Loading from: {captions_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load captions\n",
    "captions_df = pd.read_csv(captions_file)\n",
    "print(f\"Dataset shape: {captions_df.shape}\")\n",
    "print(f\"\\nColumns: {captions_df.columns.tolist()}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "captions_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistics\n",
    "num_images = len(captions_df['image'].unique())\n",
    "num_captions = len(captions_df)\n",
    "avg_captions_per_image = num_captions / num_images\n",
    "\n",
    "print(\"Dataset Statistics:\")\n",
    "print(f\"Total unique images: {num_images}\")\n",
    "print(f\"Total captions: {num_captions}\")\n",
    "print(f\"Average captions per image: {avg_captions_per_image:.2f}\")\n",
    "\n",
    "# Check caption distribution per image\n",
    "caption_counts = captions_df['image'].value_counts()\n",
    "print(f\"\\nCaptions per image distribution:\")\n",
    "print(caption_counts.value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caption Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process captions\n",
    "print(\"Processing captions...\")\n",
    "captions_df['processed_caption'] = captions_df['caption'].apply(preprocess_caption)\n",
    "captions_df['caption_length'] = captions_df['processed_caption'].apply(lambda x: len(x.split()))\n",
    "\n",
    "# Caption length statistics\n",
    "print(\"\\nCaption Length Statistics:\")\n",
    "print(captions_df['caption_length'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caption length distribution\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(captions_df['caption_length'], bins=30, edgecolor='black', alpha=0.7)\n",
    "plt.xlabel('Caption Length (words)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Caption Lengths')\n",
    "plt.grid(False)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "captions_df['caption_length'].plot(kind='box')\n",
    "plt.ylabel('Caption Length (words)')\n",
    "plt.title('Caption Length Box Plot')\n",
    "plt.grid(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word frequency analysis\n",
    "all_words = []\n",
    "for caption in captions_df['processed_caption']:\n",
    "    all_words.extend(caption.split())\n",
    "\n",
    "word_freq = Counter(all_words)\n",
    "print(f\"Total unique words: {len(word_freq)}\")\n",
    "print(f\"Total words: {len(all_words)}\")\n",
    "\n",
    "# Most common words\n",
    "print(\"\\nTop 20 most common words:\")\n",
    "for word, count in word_freq.most_common(20):\n",
    "    print(f\"{word}: {count} ({count/len(all_words)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word cloud visualization\n",
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "# All words\n",
    "plt.subplot(1, 2, 1)\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(' '.join(all_words))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.title('Word Cloud - All Words')\n",
    "plt.axis('off')\n",
    "\n",
    "# Without stopwords\n",
    "plt.subplot(1, 2, 2)\n",
    "# Filter out common words\n",
    "stopwords = {'a', 'an', 'the', 'is', 'in', 'on', 'and', 'of', 'with', 'at', 'to', 'are'}\n",
    "filtered_words = [w for w in all_words if w not in stopwords]\n",
    "wordcloud_filtered = WordCloud(width=800, height=400, background_color='white').generate(' '.join(filtered_words))\n",
    "plt.imshow(wordcloud_filtered, interpolation='bilinear')\n",
    "plt.title('Word Cloud - Content Words')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample images for analysis\n",
    "sample_images = captions_df['image'].unique()[:100]  # Analyze first 100 images\n",
    "\n",
    "# Collect image properties\n",
    "image_properties = []\n",
    "\n",
    "for img_name in tqdm(sample_images, desc=\"Analyzing images\"):\n",
    "    img_path = os.path.join(images_dir, img_name)\n",
    "    if os.path.exists(img_path):\n",
    "        img = Image.open(img_path)\n",
    "        image_properties.append({\n",
    "            'filename': img_name,\n",
    "            'width': img.width,\n",
    "            'height': img.height,\n",
    "            'aspect_ratio': img.width / img.height,\n",
    "            'mode': img.mode,\n",
    "            'format': img.format\n",
    "        })\n",
    "\n",
    "image_df = pd.DataFrame(image_properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image size distribution\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.scatter(image_df['width'], image_df['height'], alpha=0.5)\n",
    "plt.xlabel('Width (pixels)')\n",
    "plt.ylabel('Height (pixels)')\n",
    "plt.title('Image Dimensions')\n",
    "plt.grid(False)\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.hist(image_df['aspect_ratio'], bins=20, edgecolor='black', alpha=0.7)\n",
    "plt.xlabel('Aspect Ratio (width/height)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Aspect Ratio Distribution')\n",
    "plt.grid(False)\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "image_df['resolution'] = image_df['width'] * image_df['height']\n",
    "plt.hist(image_df['resolution'] / 1e6, bins=20, edgecolor='black', alpha=0.7)\n",
    "plt.xlabel('Resolution (megapixels)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Resolution Distribution')\n",
    "plt.grid(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Image Statistics:\")\n",
    "print(f\"Average width: {image_df['width'].mean():.1f} pixels\")\n",
    "print(f\"Average height: {image_df['height'].mean():.1f} pixels\")\n",
    "print(f\"Average aspect ratio: {image_df['aspect_ratio'].mean():.2f}\")\n",
    "print(f\"Image formats: {image_df['format'].value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caption-Image Relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze caption variation for same image\n",
    "sample_image = captions_df['image'].unique()[0]\n",
    "sample_captions = captions_df[captions_df['image'] == sample_image]\n",
    "\n",
    "print(f\"Sample image: {sample_image}\")\n",
    "print(f\"\\nCaptions for this image:\")\n",
    "for i, (_, row) in enumerate(sample_captions.iterrows(), 1):\n",
    "    print(f\"{i}. {row['caption']}\")\n",
    "    \n",
    "# Display the image\n",
    "img_path = os.path.join(images_dir, sample_image)\n",
    "img = Image.open(img_path)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(img)\n",
    "plt.title(f\"Image: {sample_image}\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caption diversity analysis\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Calculate caption similarity for images with multiple captions\n",
    "similarity_scores = []\n",
    "\n",
    "for img in tqdm(captions_df['image'].unique()[:100], desc=\"Calculating similarities\"):\n",
    "    img_captions = captions_df[captions_df['image'] == img]['processed_caption'].tolist()\n",
    "    if len(img_captions) > 1:\n",
    "        # Calculate TF-IDF vectors\n",
    "        vectorizer = TfidfVectorizer()\n",
    "        tfidf_matrix = vectorizer.fit_transform(img_captions)\n",
    "        \n",
    "        # Calculate pairwise similarities\n",
    "        similarities = cosine_similarity(tfidf_matrix)\n",
    "        \n",
    "        # Get upper triangle (excluding diagonal)\n",
    "        for i in range(len(img_captions)):\n",
    "            for j in range(i + 1, len(img_captions)):\n",
    "                similarity_scores.append(similarities[i, j])\n",
    "\n",
    "# Plot similarity distribution\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(similarity_scores, bins=30, edgecolor='black', alpha=0.7)\n",
    "plt.xlabel('Cosine Similarity')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Caption Similarities for Same Image')\n",
    "plt.grid(False)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Average caption similarity: {np.mean(similarity_scores):.3f}\")\n",
    "print(f\"Std caption similarity: {np.std(similarity_scores):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocabulary Analysis with Different Thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze vocabulary size with different frequency thresholds\n",
    "thresholds = [1, 2, 3, 4, 5, 10, 20]\n",
    "vocab_sizes = []\n",
    "coverage_rates = []\n",
    "\n",
    "for threshold in thresholds:\n",
    "    # Count words above threshold\n",
    "    vocab_size = sum(1 for count in word_freq.values() if count >= threshold) + 4  # +4 for special tokens\n",
    "    vocab_sizes.append(vocab_size)\n",
    "    \n",
    "    # Calculate coverage\n",
    "    covered_words = sum(count for word, count in word_freq.items() if count >= threshold)\n",
    "    coverage = covered_words / len(all_words) * 100\n",
    "    coverage_rates.append(coverage)\n",
    "\n",
    "# Plot results\n",
    "fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "color = 'tab:blue'\n",
    "ax1.set_xlabel('Frequency Threshold')\n",
    "ax1.set_ylabel('Vocabulary Size', color=color)\n",
    "ax1.plot(thresholds, vocab_sizes, 'o-', color=color, linewidth=2, markersize=8)\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "ax1.grid(False)\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "color = 'tab:red'\n",
    "ax2.set_ylabel('Word Coverage (%)', color=color)\n",
    "ax2.plot(thresholds, coverage_rates, 's-', color=color, linewidth=2, markersize=8)\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "plt.title('Vocabulary Size and Coverage vs Frequency Threshold')\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print table\n",
    "print(\"Threshold | Vocab Size | Coverage\")\n",
    "print()\n",
    "for t, v, c in zip(thresholds, vocab_sizes, coverage_rates):\n",
    "    print(f\"{t:9d} | {v:10d} | {c:7.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary report\n",
    "print(\"DATASET SUMMARY\")\n",
    "\n",
    "print(f\"\\nImages:\")\n",
    "print(f\"  Total images: {num_images}\")\n",
    "print(f\"  Average resolution: {image_df['resolution'].mean() / 1e6:.1f} MP\")\n",
    "print(f\"  Most common aspect ratio: {image_df['aspect_ratio'].mode()[0]:.2f}\")\n",
    "\n",
    "print(f\"\\nCaptions:\")\n",
    "print(f\"  Total captions: {num_captions}\")\n",
    "print(f\"  Captions per image: {avg_captions_per_image:.1f}\")\n",
    "print(f\"  Average caption length: {captions_df['caption_length'].mean():.1f} words\")\n",
    "print(f\"  Caption length range: {captions_df['caption_length'].min()}-{captions_df['caption_length'].max()} words\")\n",
    "\n",
    "print(f\"\\nVocabulary:\")\n",
    "print(f\"  Total unique words: {len(word_freq)}\")\n",
    "print(f\"  Words with freq >= 5: {sum(1 for c in word_freq.values() if c >= 5)}\")\n",
    "print(f\"  Most common word: '{word_freq.most_common(1)[0][0]}' ({word_freq.most_common(1)[0][1]} times)\")\n",
    "\n",
    "print(f\"\\nCaption Diversity:\")\n",
    "print(f\"  Average similarity between captions: {np.mean(similarity_scores):.3f}\")\n",
    "print(f\"  This indicates {'high' if np.mean(similarity_scores) > 0.7 else 'moderate' if np.mean(similarity_scores) > 0.4 else 'low'} similarity\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
