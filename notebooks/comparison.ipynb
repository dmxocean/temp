{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Captioning: Model Comparison Analysis\n",
    "\n",
    "This notebook compares the baseline and attention models:\n",
    "- Load trained models\n",
    "- Compare performance metrics\n",
    "- Analyze generated captions\n",
    "- Visualize attention mechanisms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and imports\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Add project root to path\n",
    "project_root = os.path.dirname(os.getcwd())\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "# Import project modules\n",
    "from src.utils.manager import ConfigManager\n",
    "from src.utils.constants import ROOT_DIR\n",
    "from src.utils.io import load_pickle, load_json\n",
    "from src.preprocessing.vocabulary import Vocabulary\n",
    "from src.preprocessing.dataset import FlickrDataset\n",
    "from src.preprocessing.transforms import get_transforms\n",
    "from src.models.baseline import BaselineCaptionModel\n",
    "from src.models.attention import AttentionCaptionModel\n",
    "from src.comparison.evaluator import ModelEvaluator\n",
    "from src.visualization.captioning import compare_models, plot_training_history\n",
    "from src.visualization.attention import visualize_attention\n",
    "\n",
    "# Verify we found the correct project root\n",
    "print(f\"Project root: {ROOT_DIR}\")\n",
    "print(f\"Config directory exists: {os.path.exists(os.path.join(ROOT_DIR, 'config'))}\")\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize configuration manager\n",
    "config_manager = ConfigManager()\n",
    "\n",
    "# Load vocabulary\n",
    "vocab_path = config_manager.paths['vocab']\n",
    "print(f\"Loading vocabulary from: {vocab_path}\")\n",
    "vocab = Vocabulary.load(vocab_path)\n",
    "print(f\"Vocabulary size: {len(vocab)}\")\n",
    "\n",
    "# Load data splits\n",
    "splits_path = config_manager.paths['splits']\n",
    "splits = load_pickle(splits_path)\n",
    "test_df = splits['test']\n",
    "print(f\"\\nTest set: {len(test_df)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Trained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load baseline model\n",
    "baseline_config = config_manager.get_model_config('baseline')\n",
    "baseline_model = BaselineCaptionModel(\n",
    "    embed_size=baseline_config['embed_size'],\n",
    "    hidden_size=baseline_config['hidden_size'],\n",
    "    vocab_size=len(vocab),\n",
    "    num_layers=baseline_config['num_layers'],\n",
    "    dropout=baseline_config['dropout']\n",
    ").to(device)\n",
    "\n",
    "# Load baseline weights\n",
    "baseline_checkpoint_path = os.path.join(config_manager.get_model_dir('baseline'), 'best_model.pth')\n",
    "if os.path.exists(baseline_checkpoint_path):\n",
    "    print(f\"Loading baseline model from: {baseline_checkpoint_path}\")\n",
    "    checkpoint = torch.load(baseline_checkpoint_path, map_location=device)\n",
    "    baseline_model.load_state_dict(checkpoint['state_dict'])\n",
    "    baseline_model.eval()\n",
    "    print(\"Baseline model loaded successfully\")\n",
    "else:\n",
    "    print(\"⚠ Baseline model not found. Please train it first using scripts/baseline.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load attention model\n",
    "attention_config = config_manager.get_model_config('attention')\n",
    "attention_model = AttentionCaptionModel(\n",
    "    embed_size=attention_config['embed_size'],\n",
    "    hidden_size=attention_config['hidden_size'],\n",
    "    vocab_size=len(vocab),\n",
    "    attention_dim=attention_config['attention_dim'],\n",
    "    num_layers=attention_config['num_layers'],\n",
    "    dropout=attention_config['dropout']\n",
    ").to(device)\n",
    "\n",
    "# Load attention weights\n",
    "attention_checkpoint_path = os.path.join(config_manager.get_model_dir('attention'), 'best_model.pth')\n",
    "if os.path.exists(attention_checkpoint_path):\n",
    "    print(f\"Loading attention model from: {attention_checkpoint_path}\")\n",
    "    checkpoint = torch.load(attention_checkpoint_path, map_location=device)\n",
    "    attention_model.load_state_dict(checkpoint['state_dict'])\n",
    "    attention_model.eval()\n",
    "    print(\"Attention model loaded successfully\")\n",
    "else:\n",
    "    print(\"⚠ Attention model not found. Please train it first using scripts/attention.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Training Histories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training histories\n",
    "baseline_history_path = os.path.join(config_manager.get_model_dir('baseline'), 'training_history.json')\n",
    "attention_history_path = os.path.join(config_manager.get_model_dir('attention'), 'training_history.json')\n",
    "\n",
    "baseline_history = None\n",
    "attention_history = None\n",
    "\n",
    "if os.path.exists(baseline_history_path):\n",
    "    baseline_history = load_json(baseline_history_path)\n",
    "    print(\"Loaded baseline training history\")\n",
    "\n",
    "if os.path.exists(attention_history_path):\n",
    "    attention_history = load_json(attention_history_path)\n",
    "    print(\"Loaded attention training history\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training histories\n",
    "if baseline_history:\n",
    "    print(\"\\nBaseline Model Training History:\")\n",
    "    plot_training_history(baseline_history, model_name=\"Baseline\")\n",
    "\n",
    "if attention_history:\n",
    "    print(\"\\nAttention Model Training History:\")\n",
    "    plot_training_history(attention_history, model_name=\"Attention\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test results\n",
    "baseline_results_path = os.path.join(config_manager.get_model_dir('baseline'), 'test_results.json')\n",
    "attention_results_path = os.path.join(config_manager.get_model_dir('attention'), 'test_results.json')\n",
    "\n",
    "results = {}\n",
    "\n",
    "if os.path.exists(baseline_results_path):\n",
    "    baseline_results = load_json(baseline_results_path)\n",
    "    results['Baseline'] = baseline_results['test_bleu']\n",
    "    results['Baseline']['params'] = baseline_results['model_params']\n",
    "\n",
    "if os.path.exists(attention_results_path):\n",
    "    attention_results = load_json(attention_results_path)\n",
    "    results['Attention'] = attention_results['test_bleu']\n",
    "    results['Attention']['params'] = attention_results['model_params']\n",
    "\n",
    "# Create comparison dataframe\n",
    "if results:\n",
    "    comparison_df = pd.DataFrame(results).T\n",
    "    print(\"Model Performance Comparison:\")\n",
    "    display(comparison_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize performance comparison\n",
    "if results:\n",
    "    # BLEU scores comparison\n",
    "    metrics = ['bleu1', 'bleu2', 'bleu3', 'bleu4']\n",
    "    baseline_scores = [results.get('Baseline', {}).get(m, 0) for m in metrics]\n",
    "    attention_scores = [results.get('Attention', {}).get(m, 0) for m in metrics]\n",
    "    \n",
    "    x = np.arange(len(metrics))\n",
    "    width = 0.35\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # BLEU scores bar plot\n",
    "    rects1 = ax1.bar(x - width/2, baseline_scores, width, label='Baseline', color='skyblue')\n",
    "    rects2 = ax1.bar(x + width/2, attention_scores, width, label='Attention', color='lightcoral')\n",
    "    \n",
    "    ax1.set_ylabel('Score (%)')\n",
    "    ax1.set_xlabel('Metric')\n",
    "    ax1.set_title('BLEU Score Comparison')\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels([m.upper() for m in metrics])\n",
    "    ax1.legend()\n",
    "    ax1.grid(False)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for rect in rects1 + rects2:\n",
    "        height = rect.get_height()\n",
    "        ax1.annotate(f'{height:.1f}',\n",
    "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                    xytext=(0, 3),\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom')\n",
    "    \n",
    "    # Model complexity comparison\n",
    "    if 'params' in results.get('Baseline', {}) and 'params' in results.get('Attention', {}):\n",
    "        models = ['Baseline', 'Attention']\n",
    "        params = [results['Baseline']['params'] / 1e6, results['Attention']['params'] / 1e6]\n",
    "        \n",
    "        bars = ax2.bar(models, params, color=['skyblue', 'lightcoral'])\n",
    "        ax2.set_ylabel('Parameters (millions)')\n",
    "        ax2.set_title('Model Complexity')\n",
    "        ax2.grid(False)\n",
    "        \n",
    "        # Add value labels\n",
    "        for bar, param in zip(bars, params):\n",
    "            ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,\n",
    "                    f'{param:.1f}M', ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Generated Captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test dataset\n",
    "data_config = config_manager.get_data_params()\n",
    "images_dir = data_config['dataset']['images_dir']\n",
    "\n",
    "_, transform_val = get_transforms(\n",
    "    resize=data_config['image']['resize_size'],\n",
    "    crop=data_config['image']['crop_size']\n",
    ")\n",
    "\n",
    "test_dataset = FlickrDataset(\n",
    "    data_df=test_df,\n",
    "    root_dir=images_dir,\n",
    "    vocab=vocab,\n",
    "    transform=transform_val\n",
    ")\n",
    "\n",
    "print(f\"Test dataset size: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare model outputs on sample images\n",
    "if os.path.exists(baseline_checkpoint_path) and os.path.exists(attention_checkpoint_path):\n",
    "    print(\"Comparing model outputs on sample images...\\n\")\n",
    "    compare_models(baseline_model, attention_model, test_dataset, vocab, device, num_samples=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Caption Characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize evaluator\n",
    "if os.path.exists(baseline_checkpoint_path) and os.path.exists(attention_checkpoint_path):\n",
    "    models = {\n",
    "        'Baseline': baseline_model,\n",
    "        'Attention': attention_model\n",
    "    }\n",
    "    \n",
    "    evaluator = ModelEvaluator(models, vocab, device)\n",
    "    \n",
    "    # Create data loader for analysis\n",
    "    from torch.utils.data import DataLoader\n",
    "    from src.preprocessing.dataset import FlickrCollate\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=32,\n",
    "        shuffle=False,\n",
    "        collate_fn=FlickrCollate(pad_idx=vocab.stoi[\"<PAD>\"])\n",
    "    )\n",
    "    \n",
    "    # Analyze caption lengths\n",
    "    print(\"Analyzing caption lengths...\")\n",
    "    length_stats = evaluator.analyze_caption_lengths(test_loader, max_samples=500)\n",
    "    print(\"\\nCaption Length Statistics:\")\n",
    "    display(length_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize caption length distributions\n",
    "if 'evaluator' in locals() and 'length_stats' in locals():\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # Bar plot of average lengths\n",
    "    plt.subplot(1, 2, 1)\n",
    "    models = length_stats['Model'].tolist()\n",
    "    mean_lengths = length_stats['Mean Length'].tolist()\n",
    "    std_lengths = length_stats['Std Length'].tolist()\n",
    "    \n",
    "    bars = plt.bar(models, mean_lengths, yerr=std_lengths, capsize=5,\n",
    "                   color=['gray', 'skyblue', 'lightcoral'])\n",
    "    plt.ylabel('Caption Length (words)')\n",
    "    plt.title('Average Caption Lengths')\n",
    "    plt.grid(False)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, mean in zip(bars, mean_lengths):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,\n",
    "                f'{mean:.1f}', ha='center', va='bottom')\n",
    "    \n",
    "    # Length range plot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    min_lengths = length_stats['Min Length'].tolist()\n",
    "    max_lengths = length_stats['Max Length'].tolist()\n",
    "    \n",
    "    x = np.arange(len(models))\n",
    "    width = 0.35\n",
    "    \n",
    "    plt.bar(x - width/2, min_lengths, width, label='Min', color='lightblue')\n",
    "    plt.bar(x + width/2, max_lengths, width, label='Max', color='lightcoral')\n",
    "    \n",
    "    plt.ylabel('Caption Length (words)')\n",
    "    plt.title('Caption Length Range')\n",
    "    plt.xticks(x, models)\n",
    "    plt.legend()\n",
    "    plt.grid(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize attention for sample images\n",
    "if os.path.exists(attention_checkpoint_path):\n",
    "    print(\"Visualizing attention mechanism...\\n\")\n",
    "    \n",
    "    # Select random samples\n",
    "    sample_indices = np.random.choice(len(test_dataset), 3, replace=False)\n",
    "    \n",
    "    for idx in sample_indices:\n",
    "        # Get image\n",
    "        image, _ = test_dataset[idx]\n",
    "        image = image.unsqueeze(0).to(device)\n",
    "        \n",
    "        # Generate caption with attention\n",
    "        caption, attention_weights = attention_model.caption_image_with_attention(image, vocab)\n",
    "        \n",
    "        # Denormalize image for display\n",
    "        from src.preprocessing.transforms import denormalize_image\n",
    "        img_display = denormalize_image(image[0])\n",
    "        \n",
    "        print(f\"\\nGenerated caption: {caption}\")\n",
    "        \n",
    "        # Visualize attention\n",
    "        visualize_attention(img_display, caption.split(), attention_weights, show_every=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary report\n",
    "print(\"MODEL COMPARISON SUMMARY\")\n",
    "print()\n",
    "\n",
    "if results:\n",
    "    print(\"\\nPerformance Metrics:\")\n",
    "    for model in ['Baseline', 'Attention']:\n",
    "        if model in results:\n",
    "            print(f\"\\n{model} Model:\")\n",
    "            print(f\"  BLEU-1: {results[model]['bleu1']:.2f}%\")\n",
    "            print(f\"  BLEU-2: {results[model]['bleu2']:.2f}%\")\n",
    "            print(f\"  BLEU-3: {results[model]['bleu3']:.2f}%\")\n",
    "            print(f\"  BLEU-4: {results[model]['bleu4']:.2f}%\")\n",
    "            print(f\"  Parameters: {results[model]['params']:,}\")\n",
    "    \n",
    "    # Calculate improvements\n",
    "    if 'Baseline' in results and 'Attention' in results:\n",
    "        print(\"\\nAttention Model Improvements:\")\n",
    "        for metric in ['bleu1', 'bleu2', 'bleu3', 'bleu4']:\n",
    "            baseline_score = results['Baseline'][metric]\n",
    "            attention_score = results['Attention'][metric]\n",
    "            improvement = (attention_score - baseline_score) / baseline_score * 100\n",
    "            print(f\"  {metric.upper()}: {improvement:+.1f}%\")\n",
    "\n",
    "print(\"\\nKey Findings:\")\n",
    "print(\"1. The attention mechanism helps the model focus on relevant image regions\")\n",
    "print(\"2. Attention model generally produces more accurate and detailed captions\")\n",
    "print(\"3. Attention weights provide interpretable insights into model behavior\")\n",
    "print(\"4. Trade-off: Attention model has more parameters and is slower to train\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
