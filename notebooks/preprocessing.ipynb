{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Captioning: Data Preprocessing Pipeline\n",
    "\n",
    "This notebook runs the preprocessing pipeline:\n",
    "1. Load and clean caption data\n",
    "2. Create train/validation/test splits\n",
    "3. Build vocabulary\n",
    "4. Save preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and imports\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# Add project root to path\n",
    "project_root = os.path.dirname(os.getcwd())\n",
    "sys.path.append(project_root)\n",
    "\n",
    "# Import project modules\n",
    "from src.utils.manager import ConfigManager\n",
    "from src.utils.constants import SEED\n",
    "from src.utils.io import save_pickle, ensure_dir\n",
    "from src.preprocessing.vocabulary import Vocabulary, preprocess_caption, analyze_vocab_coverage\n",
    "from src.preprocessing.dataset import create_data_splits\n",
    "from src.preprocessing.transforms import get_transforms, denormalize_image\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize configuration manager\n",
    "config_manager = ConfigManager()\n",
    "\n",
    "# Get configurations\n",
    "data_config = config_manager.get_data_params()\n",
    "debug_mode = config_manager.debug\n",
    "\n",
    "print(f\"Debug mode: {debug_mode}\")\n",
    "if debug_mode:\n",
    "    print(f\"Max images in debug mode: {data_config['debug']['max_images']}\")\n",
    "    print(f\"Output directory: {config_manager.paths['processed']}\")\n",
    "\n",
    "# Ensure output directories exist\n",
    "ensure_dir(config_manager.paths['processed'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Process Captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load captions\n",
    "captions_file = data_config['dataset']['captions_file']\n",
    "images_dir = data_config['dataset']['images_dir']\n",
    "\n",
    "print(f\"Loading captions from: {captions_file}\")\n",
    "captions_df = pd.read_csv(captions_file)\n",
    "print(f\"Loaded {len(captions_df)} captions\")\n",
    "\n",
    "# If debug mode, limit dataset\n",
    "if debug_mode:\n",
    "    max_images = data_config['debug']['max_images']\n",
    "    unique_images = captions_df['image'].unique()[:max_images]\n",
    "    captions_df = captions_df[captions_df['image'].isin(unique_images)].reset_index(drop=True)\n",
    "    print(f\"\\nDEBUG MODE: Limited to {len(unique_images)} images, {len(captions_df)} captions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process captions\n",
    "print(\"\\nProcessing captions...\")\n",
    "captions_df['processed_caption'] = captions_df['caption'].apply(preprocess_caption)\n",
    "\n",
    "# Show examples\n",
    "print(\"\\nSample processed captions:\")\n",
    "for i in range(min(3, len(captions_df))):\n",
    "    print(f\"Original: {captions_df.iloc[i]['caption']}\")\n",
    "    print(f\"Processed: {captions_df.iloc[i]['processed_caption']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caption length analysis\n",
    "captions_df['caption_length'] = captions_df['processed_caption'].apply(lambda x: len(x.split()))\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(captions_df['caption_length'], bins=20, edgecolor='black', alpha=0.7)\n",
    "plt.xlabel('Caption Length (words)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Caption Lengths')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Caption length statistics:\")\n",
    "print(captions_df['caption_length'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Data Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create splits\n",
    "train_ratio = data_config['preprocessing']['train_split']\n",
    "val_ratio = data_config['preprocessing']['val_split']\n",
    "test_ratio = data_config['preprocessing']['test_split']\n",
    "\n",
    "print(f\"Creating splits: train={train_ratio}, val={val_ratio}, test={test_ratio}\")\n",
    "\n",
    "train_df, val_df, test_df = create_data_splits(\n",
    "    captions_df, \n",
    "    train_ratio=train_ratio,\n",
    "    val_ratio=val_ratio,\n",
    "    test_ratio=test_ratio,\n",
    "    random_state=SEED\n",
    ")\n",
    "\n",
    "print(f\"\\nSplit sizes:\")\n",
    "print(f\"Train: {len(train_df)} captions, {len(train_df['image'].unique())} images\")\n",
    "print(f\"Val: {len(val_df)} captions, {len(val_df['image'].unique())} images\")\n",
    "print(f\"Test: {len(test_df)} captions, {len(test_df['image'].unique())} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify no overlap between splits\n",
    "train_images = set(train_df['image'].unique())\n",
    "val_images = set(val_df['image'].unique())\n",
    "test_images = set(test_df['image'].unique())\n",
    "\n",
    "print(\"Checking for overlaps between splits:\")\n",
    "print(f\"Train-Val overlap: {len(train_images & val_images)} images\")\n",
    "print(f\"Train-Test overlap: {len(train_images & test_images)} images\")\n",
    "print(f\"Val-Test overlap: {len(val_images & test_images)} images\")\n",
    "\n",
    "assert len(train_images & val_images) == 0, \"Train and validation sets overlap!\"\n",
    "assert len(train_images & test_images) == 0, \"Train and test sets overlap!\"\n",
    "assert len(val_images & test_images) == 0, \"Validation and test sets overlap!\"\n",
    "print(\"\\nâœ“ No overlaps found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build vocabulary from training set only\n",
    "vocab_threshold = data_config['preprocessing']['vocab_threshold']\n",
    "print(f\"Building vocabulary with frequency threshold: {vocab_threshold}\")\n",
    "\n",
    "vocab = Vocabulary(freq_threshold=vocab_threshold)\n",
    "vocab.build_vocabulary(train_df['processed_caption'].tolist())\n",
    "\n",
    "# Show vocabulary statistics\n",
    "print(f\"\\nVocabulary statistics:\")\n",
    "print(f\"Total unique words seen: {len(vocab.word_frequencies)}\")\n",
    "print(f\"Words in vocabulary: {len(vocab) - 4}\")\n",
    "print(f\"Total vocabulary size (with special tokens): {len(vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze vocabulary coverage\n",
    "print(\"\\nVocabulary coverage analysis:\")\n",
    "print(\"\\nTraining set:\")\n",
    "train_coverage, _ = analyze_vocab_coverage(train_df, vocab)\n",
    "\n",
    "print(\"\\nValidation set:\")\n",
    "val_coverage, _ = analyze_vocab_coverage(val_df, vocab)\n",
    "\n",
    "print(\"\\nTest set:\")\n",
    "test_coverage, _ = analyze_vocab_coverage(test_df, vocab)\n",
    "\n",
    "print(f\"\\nSummary:\")\n",
    "print(f\"Train coverage: {train_coverage:.2f}%\")\n",
    "print(f\"Val coverage: {val_coverage:.2f}%\")\n",
    "print(f\"Test coverage: {test_coverage:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show most frequent words\n",
    "print(\"\\nMost frequent words in vocabulary:\")\n",
    "most_freq = vocab.get_most_frequent_words(20)\n",
    "for i, (word, count) in enumerate(most_freq[:20], 1):\n",
    "    print(f\"{i:2d}. '{word}': {count} times\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Dataset Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test dataset creation\n",
    "import torch\n",
    "from src.preprocessing.dataset import FlickrDataset, FlickrCollate\n",
    "\n",
    "# Get transforms\n",
    "transform_train, transform_val = get_transforms(\n",
    "    resize=data_config['image']['resize_size'],\n",
    "    crop=data_config['image']['crop_size']\n",
    ")\n",
    "\n",
    "# Create small test dataset\n",
    "test_dataset = FlickrDataset(\n",
    "    data_df=train_df.iloc[:10],\n",
    "    root_dir=images_dir,\n",
    "    vocab=vocab,\n",
    "    transform=transform_val\n",
    ")\n",
    "\n",
    "print(f\"Test dataset size: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test data loading\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=False,\n",
    "    collate_fn=FlickrCollate(pad_idx=vocab.stoi[\"<PAD>\"])\n",
    ")\n",
    "\n",
    "# Get one batch\n",
    "images, captions, lengths = next(iter(test_loader))\n",
    "\n",
    "print(f\"Batch shapes:\")\n",
    "print(f\"Images: {images.shape}\")\n",
    "print(f\"Captions: {captions.shape}\")\n",
    "print(f\"Lengths: {lengths}\")\n",
    "\n",
    "# Display batch\n",
    "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "for i in range(4):\n",
    "    # Denormalize and display image\n",
    "    img = denormalize_image(images[i])\n",
    "    axes[i].imshow(img)\n",
    "    \n",
    "    # Decode caption\n",
    "    caption_idx = captions[i].tolist()\n",
    "    caption_words = []\n",
    "    for idx in caption_idx:\n",
    "        token = vocab.itos[idx]\n",
    "        if token == \"<EOS>\":\n",
    "            break\n",
    "        if token not in [\"<PAD>\", \"< SOS >\"]:\n",
    "            caption_words.append(token)\n",
    "    \n",
    "    axes[i].set_title(' '.join(caption_words), fontsize=10)\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save vocabulary\n",
    "vocab_path = config_manager.paths['vocab']\n",
    "vocab.save(vocab_path)\n",
    "print(f\"Saved vocabulary to: {vocab_path}\")\n",
    "\n",
    "# Save data splits\n",
    "splits_path = config_manager.paths['splits']\n",
    "splits = {\n",
    "    'train': train_df,\n",
    "    'val': val_df,\n",
    "    'test': test_df\n",
    "}\n",
    "save_pickle(splits, splits_path)\n",
    "print(f\"Saved data splits to: {splits_path}\")\n",
    "\n",
    "# Save summary\n",
    "summary = {\n",
    "    'dataset': data_config['dataset']['name'],\n",
    "    'debug_mode': debug_mode,\n",
    "    'vocab_size': len(vocab),\n",
    "    'vocab_threshold': vocab_threshold,\n",
    "    'train_size': len(train_df),\n",
    "    'val_size': len(val_df),\n",
    "    'test_size': len(test_df),\n",
    "    'train_images': len(train_df['image'].unique()),\n",
    "    'val_images': len(val_df['image'].unique()),\n",
    "    'test_images': len(test_df['image'].unique()),\n",
    "    'train_coverage': train_coverage,\n",
    "    'val_coverage': val_coverage,\n",
    "    'test_coverage': test_coverage\n",
    "}\n",
    "\n",
    "from src.utils.io import save_json\n",
    "summary_path = os.path.join(config_manager.paths['processed'], 'preprocessing_summary.json')\n",
    "save_json(summary, summary_path)\n",
    "print(f\"\\nSaved preprocessing summary to: {summary_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print final summary\n",
    "print(\"\\nPREPROCESSING COMPLETE\")\n",
    "print()\n",
    "print(f\"Debug mode: {debug_mode}\")\n",
    "print(f\"Output directory: {config_manager.paths['processed']}\")\n",
    "print(f\"\\nDataset:\")\n",
    "print(f\"  Vocabulary size: {len(vocab)}\")\n",
    "print(f\"  Training samples: {len(train_df)}\")\n",
    "print(f\"  Validation samples: {len(val_df)}\")\n",
    "print(f\"  Test samples: {len(test_df)}\")\n",
    "print(f\"\\nCoverage:\")\n",
    "print(f\"  Train: {train_coverage:.2f}%\")\n",
    "print(f\"  Val: {val_coverage:.2f}%\")\n",
    "print(f\"  Test: {test_coverage:.2f}%\")\n",
    "print(f\"\\nFiles saved:\")\n",
    "print(f\"  - {vocab_path}\")\n",
    "print(f\"  - {splits_path}\")\n",
    "print(f\"  - {summary_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}