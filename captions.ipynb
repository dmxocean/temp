{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "612c461c",
   "metadata": {},
   "source": [
    "# Project Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f873cd2",
   "metadata": {},
   "source": [
    "After running the code above, you'll gain insights into the dataset structure, caption statistics, and visualize\n",
    "sample images with their captions. This is our first step in understanding the data we'll be working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee74112e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Captioning with CNN+RNN and Attention Models\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa73f1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setup and Configuration\n",
    "\n",
    "### 1.1 Imports and Environment Setup\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "import traceback\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Deep learning imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# NLP imports\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import corpus_bleu, sentence_bleu\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "\n",
    "try:\n",
    "    import spacy\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "except (ImportError, OSError):\n",
    "    print(\"Installing spaCy model...\")\n",
    "    spacy.cli.download(\"en_core_web_sm\")\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# Check device availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "### 1.2 Configuration Parameters and Paths\n",
    "# Set execution mode - set to False for full training\n",
    "DEBUG_MODE = False\n",
    "\n",
    "# Root directory and paths setup\n",
    "ROOT_DIR = os.path.dirname(os.getcwd())  # Project root directory\n",
    "DATA_DIR = os.path.join(ROOT_DIR, \"data\")\n",
    "OUTPUT_DIR = os.path.join(ROOT_DIR, \"output\")\n",
    "MODEL_DIR = os.path.join(OUTPUT_DIR, \"models\")\n",
    "LOGS_DIR = os.path.join(OUTPUT_DIR, \"logs\")\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "os.makedirs(LOGS_DIR, exist_ok=True)\n",
    "\n",
    "# Dataset paths\n",
    "FLICKR8K_DIR = os.path.join(DATA_DIR, \"raw\", \"flickr8k\")\n",
    "IMAGES_DIR = os.path.join(FLICKR8K_DIR, \"images\")\n",
    "CAPTIONS_FILE = os.path.join(FLICKR8K_DIR, \"captions.txt\")\n",
    "\n",
    "# ImageNet normalization constants\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "# Verify file paths\n",
    "print(f\"Images directory exists: {os.path.exists(IMAGES_DIR)}\")\n",
    "print(f\"Captions file exists: {os.path.exists(CAPTIONS_FILE)}\")\n",
    "\n",
    "### 1.3 Model Configuration Parameters\n",
    "# Shared model configuration parameters\n",
    "model_config = {\n",
    "    # Architecture\n",
    "    \"embed_size\": 256,          # Embedding dimension\n",
    "    \"hidden_size\": 512,         # LSTM hidden state size\n",
    "    \"attention_dim\": 256,       # Attention network dimension\n",
    "    \"num_layers\": 1,            # Number of LSTM layers\n",
    "    \"dropout\": 0.5,             # Dropout probability\n",
    "    \n",
    "    # Training\n",
    "    \"num_epochs\": 10 if not DEBUG_MODE else 3,\n",
    "    \"batch_size\": 32 if not DEBUG_MODE else 8,\n",
    "    \"learning_rate\": 3e-4,\n",
    "    \"weight_decay\": 1e-5,\n",
    "    \"clip_grad_norm\": 5.0,\n",
    "    \n",
    "    # Scheduler\n",
    "    \"use_lr_scheduler\": True,\n",
    "    \"lr_scheduler_factor\": 0.5,\n",
    "    \"lr_scheduler_patience\": 2,\n",
    "    \n",
    "    # Evaluation\n",
    "    \"eval_every\": 1,            # Validate every N epochs\n",
    "    \"bleu_every\": 2,            # Calculate BLEU every N epochs\n",
    "    \"max_bleu_samples\": None if not DEBUG_MODE else 50,\n",
    "    \n",
    "    # Early stopping\n",
    "    \"early_stopping_patience\": 5,\n",
    "    \n",
    "    # Logging and checkpoints\n",
    "    \"print_frequency\": 50 if not DEBUG_MODE else 5,\n",
    "    \"save_best_only\": True,\n",
    "    \"save_frequency\": 1\n",
    "}\n",
    "\n",
    "# Paths for baseline model\n",
    "baseline_paths = {\n",
    "    \"checkpoint_path\": os.path.join(MODEL_DIR, \"baseline_model.pth\"),\n",
    "    \"best_model_path\": os.path.join(MODEL_DIR, \"baseline_model_best.pth\")\n",
    "}\n",
    "\n",
    "# Paths for attention model\n",
    "attention_paths = {\n",
    "    \"checkpoint_path\": os.path.join(MODEL_DIR, \"attention_model.pth\"),\n",
    "    \"best_model_path\": os.path.join(MODEL_DIR, \"attention_model_best.pth\")\n",
    "}\n",
    "\n",
    "# Debug paths\n",
    "if DEBUG_MODE:\n",
    "    baseline_paths = {\n",
    "        \"checkpoint_path\": os.path.join(MODEL_DIR, \"baseline_model_debug.pth\"),\n",
    "        \"best_model_path\": os.path.join(MODEL_DIR, \"baseline_model_debug_best.pth\")\n",
    "    }\n",
    "    attention_paths = {\n",
    "        \"checkpoint_path\": os.path.join(MODEL_DIR, \"attention_model_debug.pth\"),\n",
    "        \"best_model_path\": os.path.join(MODEL_DIR, \"attention_model_debug_best.pth\")\n",
    "    }\n",
    "\n",
    "### 1.4 Utility Functions\n",
    "def debug_print(message, tensor=None, level=0):\n",
    "    \"\"\"Print debug message with optional tensor information\"\"\"\n",
    "    indent = \"  \" * level\n",
    "    print(f\"{indent}DEBUG: {message}\")\n",
    "\n",
    "    if tensor is not None and isinstance(tensor, torch.Tensor):\n",
    "        print(f\"{indent}  Shape: {tensor.shape}\")\n",
    "        print(f\"{indent}  Type: {tensor.dtype}\")\n",
    "        print(f\"{indent}  Device: {tensor.device}\")\n",
    "        print(f\"{indent}  Values - Min: {tensor.min().item():.4f}, Max: {tensor.max().item():.4f}, Mean: {tensor.mean().item():.4f}\")\n",
    "        print(f\"{indent}  Requires grad: {tensor.requires_grad}\")\n",
    "\n",
    "class AverageMeter:\n",
    "    \"\"\"Compute and store the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "def normalize_image(image):\n",
    "    \"\"\"Normalize an image using ImageNet statistics\"\"\"\n",
    "    # Create transform\n",
    "    normalize = transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
    "\n",
    "    # Apply normalization\n",
    "    if image.dim() == 3:  # Single image\n",
    "        return normalize(image)\n",
    "    else:  # Batch of images\n",
    "        return torch.stack([normalize(img) for img in image])\n",
    "\n",
    "def denormalize_image(image_tensor):\n",
    "    \"\"\"Reverse ImageNet normalization for visualization\"\"\"\n",
    "    # Convert to numpy and move channels to the end\n",
    "    if image_tensor.dim() == 4:  # Batch of images\n",
    "        image = image_tensor[0].cpu().permute(1, 2, 0).numpy()\n",
    "    else:  # Single image\n",
    "        image = image_tensor.cpu().permute(1, 2, 0).numpy()\n",
    "\n",
    "    # Reverse normalization\n",
    "    image = image * np.array(IMAGENET_STD).reshape(1, 1, 3) + np.array(IMAGENET_MEAN).reshape(1, 1, 3)\n",
    "\n",
    "    # Clip values\n",
    "    image = np.clip(image, 0, 1)\n",
    "    return image\n",
    "\n",
    "def save_checkpoint(state, is_best=False, filepath='checkpoint.pth'):\n",
    "    \"\"\"Save a checkpoint of the model\"\"\"\n",
    "    # Create directory if it doesn't exist\n",
    "    os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
    "\n",
    "    # Save checkpoint\n",
    "    torch.save(state, filepath)\n",
    "\n",
    "    # If this is the best model, print message\n",
    "    if is_best:\n",
    "        print(f\"Saved best model to {filepath}\")\n",
    "    else:\n",
    "        print(f\"Saved checkpoint to {filepath}\")\n",
    "\n",
    "def check_model_availability(config, best_model_path, checkpoint_path):\n",
    "    \"\"\"Check if a trained model or checkpoint exists and is compatible\"\"\"\n",
    "    # Check for fully trained model\n",
    "    if os.path.exists(best_model_path):\n",
    "        try:\n",
    "            model_checkpoint = torch.load(best_model_path)\n",
    "            print(f\"Found trained model at {best_model_path}\")\n",
    "            return \"trained\", model_checkpoint\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading trained model: {e}\")\n",
    "    \n",
    "    # Check for checkpoint\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        try:\n",
    "            checkpoint = torch.load(checkpoint_path)\n",
    "            print(f\"Found checkpoint at {checkpoint_path}\")\n",
    "            return \"checkpoint\", checkpoint\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading checkpoint: {e}\")\n",
    "    \n",
    "    # No compatible model or checkpoint found\n",
    "    print(\"No trained model or checkpoint found. Will train a new model.\")\n",
    "    return \"train_new\", None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ed0b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data Pipeline\n",
    "\n",
    "### 2.1 Dataset Loading and Exploration\n",
    "# Load captions data\n",
    "captions_df = pd.read_csv(CAPTIONS_FILE)\n",
    "print(\"\\nDataset Overview:\")\n",
    "print(f\"Captions dataframe shape: {captions_df.shape}\")\n",
    "\n",
    "# Calculate basic dataset statistics\n",
    "num_images = len(captions_df['image'].unique())\n",
    "num_captions = len(captions_df)\n",
    "avg_captions_per_image = num_captions / num_images\n",
    "\n",
    "print(\"\\nDataset Statistics:\")\n",
    "print(f\"Number of unique images: {num_images}\")\n",
    "print(f\"Total number of captions: {num_captions}\")\n",
    "print(f\"Average captions per image: {avg_captions_per_image:.2f}\")\n",
    "\n",
    "# Display sample data\n",
    "print(\"\\nSample captions:\")\n",
    "for i in range(min(5, len(captions_df))):\n",
    "    print(f\"Image: {captions_df.iloc[i]['image']}\")\n",
    "    print(f\"Caption: {captions_df.iloc[i]['caption']}\\n\")\n",
    "\n",
    "### 2.2 Data Preprocessing\n",
    "def preprocess_caption(caption):\n",
    "    \"\"\"Preprocess caption text\"\"\"\n",
    "    # Convert to lowercase\n",
    "    caption = caption.lower()\n",
    "\n",
    "    # Tokenize using spaCy for better handling of punctuation\n",
    "    tokens = [token.text for token in nlp.tokenizer(caption)]\n",
    "\n",
    "    # Join tokens back to string\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# Apply preprocessing to captions\n",
    "print(\"Preprocessing captions...\")\n",
    "captions_df['processed_caption'] = captions_df['caption'].apply(preprocess_caption)\n",
    "\n",
    "# Display sample preprocessed captions\n",
    "print(\"\\nSample of original vs. processed captions:\")\n",
    "for i in range(min(5, len(captions_df))):\n",
    "    original = captions_df.iloc[i]['caption']\n",
    "    processed = captions_df.iloc[i]['processed_caption']\n",
    "    print(f\"Original: {original}\")\n",
    "    print(f\"Processed: {processed}\\n\")\n",
    "\n",
    "### 2.3 Data Splitting\n",
    "def create_data_splits(df, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15, random_state=42):\n",
    "    \"\"\"Split images into train, validation, and test sets using stratified sampling\"\"\"\n",
    "    # Verify ratios sum to 1\n",
    "    assert abs(train_ratio + val_ratio + test_ratio - 1.0) < 1e-10, \"Ratios must sum to 1\"\n",
    "\n",
    "    # Get unique image IDs\n",
    "    unique_images = df['image'].unique()\n",
    "    \n",
    "    # Create stratification features for better distribution\n",
    "    strat_features = {}\n",
    "    \n",
    "    # Caption length feature (short, medium, long)\n",
    "    caption_lengths = {}\n",
    "    for img in unique_images:\n",
    "        img_captions = df[df['image'] == img]['processed_caption']\n",
    "        avg_len = sum(len(cap.split()) for cap in img_captions) / len(img_captions)\n",
    "        caption_lengths[img] = avg_len\n",
    "    \n",
    "    # Determine caption length categories\n",
    "    caption_lens = np.array(list(caption_lengths.values()))\n",
    "    q1, q2 = np.percentile(caption_lens, [33, 66])\n",
    "    \n",
    "    for img, length in caption_lengths.items():\n",
    "        if length <= q1:\n",
    "            strat_features[img] = 'short'\n",
    "        elif length <= q2:\n",
    "            strat_features[img] = 'medium'\n",
    "        else:\n",
    "            strat_features[img] = 'long'\n",
    "    \n",
    "    # Create arrays for stratified split\n",
    "    image_array = np.array(list(strat_features.keys()))\n",
    "    strat_array = np.array([strat_features[img] for img in image_array])\n",
    "    \n",
    "    # Use stratified split\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    # First split: train+val vs test\n",
    "    train_val_imgs, test_imgs, _, _ = train_test_split(\n",
    "        image_array, strat_array,\n",
    "        test_size=test_ratio,\n",
    "        random_state=random_state,\n",
    "        stratify=strat_array\n",
    "    )\n",
    "    \n",
    "    # Second split: train vs val\n",
    "    # Recalculate stratification features for the train+val set\n",
    "    strat_array_train_val = np.array([strat_features[img] for img in train_val_imgs])\n",
    "    \n",
    "    # Split train+val into train and val\n",
    "    val_ratio_adjusted = val_ratio / (train_ratio + val_ratio)\n",
    "    train_imgs, val_imgs, _, _ = train_test_split(\n",
    "        train_val_imgs, strat_array_train_val,\n",
    "        test_size=val_ratio_adjusted,\n",
    "        random_state=random_state,\n",
    "        stratify=strat_array_train_val\n",
    "    )\n",
    "    \n",
    "    # Create dataframes for each split\n",
    "    train_df = df[df['image'].isin(train_imgs)].reset_index(drop=True)\n",
    "    val_df = df[df['image'].isin(val_imgs)].reset_index(drop=True)\n",
    "    test_df = df[df['image'].isin(test_imgs)].reset_index(drop=True)\n",
    "    \n",
    "    # Verify stratification worked\n",
    "    print(\"Stratification verification:\")\n",
    "    for caption_type in ['short', 'medium', 'long']:\n",
    "        train_count = sum(1 for img in train_imgs if strat_features[img] == caption_type)\n",
    "        val_count = sum(1 for img in val_imgs if strat_features[img] == caption_type)\n",
    "        test_count = sum(1 for img in test_imgs if strat_features[img] == caption_type)\n",
    "        \n",
    "        print(f\"  {caption_type.capitalize()} captions - \"\n",
    "              f\"Train: {train_count/len(train_imgs)*100:.1f}%, \"\n",
    "              f\"Val: {val_count/len(val_imgs)*100:.1f}%, \"\n",
    "              f\"Test: {test_count/len(test_imgs)*100:.1f}%\")\n",
    "\n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "# Create data splits\n",
    "train_df, val_df, test_df = create_data_splits(captions_df)\n",
    "\n",
    "print(\"Data splits:\")\n",
    "print(f\"Training: {len(train_df)} captions, {len(train_df['image'].unique())} images\")\n",
    "print(f\"Validation: {len(val_df)} captions, {len(val_df['image'].unique())} images\")\n",
    "print(f\"Testing: {len(test_df)} captions, {len(test_df['image'].unique())} images\")\n",
    "\n",
    "### 2.4 Vocabulary Building\n",
    "class Vocabulary:\n",
    "    \"\"\"Vocabulary class to handle text tokenization and numericalization\"\"\"\n",
    "\n",
    "    def __init__(self, freq_threshold=5):\n",
    "        # Special tokens\n",
    "        self.itos = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\"}\n",
    "        self.stoi = {\"<PAD>\": 0, \"<SOS>\": 1, \"<EOS>\": 2, \"<UNK>\": 3}\n",
    "\n",
    "        # Frequency threshold to include a word in vocabulary\n",
    "        self.freq_threshold = freq_threshold\n",
    "\n",
    "        # Counter for new indices\n",
    "        self.idx = 4\n",
    "        \n",
    "        # For storing word frequencies\n",
    "        self.word_frequencies = {}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.itos)\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        \"\"\"Tokenize text using spaCy\"\"\"\n",
    "        return [token.text.lower() for token in nlp.tokenizer(text)]\n",
    "\n",
    "    def build_vocabulary(self, captions):\n",
    "        \"\"\"Build vocabulary from a list of captions\"\"\"\n",
    "        # Counter for word frequencies\n",
    "        frequencies = {}\n",
    "\n",
    "        print(f\"Building vocabulary from {len(captions)} captions...\")\n",
    "\n",
    "        # Process all captions\n",
    "        for caption in tqdm(captions):\n",
    "            # Tokenize caption\n",
    "            for word in self.tokenize(caption):\n",
    "                # Update frequency counter\n",
    "                frequencies[word] = frequencies.get(word, 0) + 1\n",
    "\n",
    "                # Add word to vocab if it reaches threshold\n",
    "                if frequencies[word] == self.freq_threshold:\n",
    "                    self.stoi[word] = self.idx\n",
    "                    self.itos[self.idx] = word\n",
    "                    self.idx += 1\n",
    "\n",
    "        print(f\"Built vocabulary with {len(self.itos)} tokens\")\n",
    "        print(f\"Added {len(self.itos) - 4} words above frequency threshold {self.freq_threshold}\")\n",
    "\n",
    "        # Save word frequencies for later analysis\n",
    "        self.word_frequencies = frequencies\n",
    "        return frequencies\n",
    "\n",
    "    def numericalize(self, text):\n",
    "        \"\"\"Convert text to sequence of indices\"\"\"\n",
    "        tokenized = self.tokenize(text)\n",
    "        return [\n",
    "            self.stoi.get(token, self.stoi[\"<UNK>\"])\n",
    "            for token in tokenized\n",
    "        ]\n",
    "\n",
    "# Build vocabulary from training set\n",
    "vocab = Vocabulary(freq_threshold=5)\n",
    "word_frequencies = vocab.build_vocabulary(train_df['processed_caption'].tolist())\n",
    "\n",
    "# Analyze vocabulary coverage\n",
    "def analyze_vocab_coverage(df, vocab, caption_col='processed_caption'):\n",
    "    \"\"\"Analyze what percentage of words in the dataset are covered by the vocabulary\"\"\"\n",
    "    total_words = 0\n",
    "    unknown_words = 0\n",
    "    unknown_word_instances = {}\n",
    "\n",
    "    for caption in df[caption_col]:\n",
    "        tokens = vocab.tokenize(caption)\n",
    "        total_words += len(tokens)\n",
    "\n",
    "        for token in tokens:\n",
    "            if token not in vocab.stoi:\n",
    "                unknown_words += 1\n",
    "                unknown_word_instances[token] = unknown_word_instances.get(token, 0) + 1\n",
    "\n",
    "    coverage = (total_words - unknown_words) / total_words * 100\n",
    "\n",
    "    print(f\"Vocabulary coverage: {coverage:.2f}%\")\n",
    "    print(f\"Total words: {total_words}\")\n",
    "    print(f\"Unknown words: {unknown_words}\")\n",
    "\n",
    "    if unknown_words > 0:\n",
    "        print(\"\\nTop unknown words:\")\n",
    "        for word, count in sorted(unknown_word_instances.items(), key=lambda x: x[1], reverse=True)[:10]:\n",
    "            print(f\"  {word}: {count} occurrences\")\n",
    "\n",
    "    return coverage, unknown_word_instances\n",
    "\n",
    "print(\"\\nAnalyzing vocabulary coverage:\")\n",
    "train_coverage, _ = analyze_vocab_coverage(train_df, vocab)\n",
    "val_coverage, _ = analyze_vocab_coverage(val_df, vocab)\n",
    "test_coverage, _ = analyze_vocab_coverage(test_df, vocab)\n",
    "\n",
    "print(f\"\\nSummary:\")\n",
    "print(f\"Train coverage: {train_coverage:.2f}%\")\n",
    "print(f\"Validation coverage: {val_coverage:.2f}%\")\n",
    "print(f\"Test coverage: {test_coverage:.2f}%\")\n",
    "\n",
    "### 2.5 Image Processing\n",
    "def get_transforms(resize=256, crop=224):\n",
    "    \"\"\"Create image transformations for training and validation/test sets\"\"\"\n",
    "    # Training transforms with data augmentation\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.Resize(resize),\n",
    "        transforms.RandomCrop(crop),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "        transforms.RandomRotation(10),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
    "    ])\n",
    "\n",
    "    # Validation/test transforms (no augmentation)\n",
    "    transform_val = transforms.Compose([\n",
    "        transforms.Resize(resize),\n",
    "        transforms.CenterCrop(crop),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
    "    ])\n",
    "\n",
    "    return transform_train, transform_val\n",
    "\n",
    "# Create transforms\n",
    "transform_train, transform_val = get_transforms()\n",
    "\n",
    "### 2.6 Dataset and DataLoader\n",
    "class FlickrDataset(Dataset):\n",
    "    \"\"\"Dataset class for Flickr8k images and captions\"\"\"\n",
    "\n",
    "    def __init__(self, data_df, root_dir, vocab, transform=None, caption_col='processed_caption'):\n",
    "        \"\"\"\n",
    "        Initialize the dataset.\n",
    "\n",
    "        Args:\n",
    "            data_df: DataFrame containing image filenames and captions\n",
    "            root_dir: Directory containing the images\n",
    "            vocab: Vocabulary object for processing captions\n",
    "            transform: Optional image transformations\n",
    "            caption_col: Column name for captions in data_df\n",
    "        \"\"\"\n",
    "        self.data_df = data_df\n",
    "        self.root_dir = root_dir\n",
    "        self.vocab = vocab\n",
    "        self.transform = transform\n",
    "        self.caption_col = caption_col\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Get an image and its corresponding caption\"\"\"\n",
    "        # Get caption and image path\n",
    "        caption = self.data_df.iloc[idx][self.caption_col]\n",
    "        img_name = self.data_df.iloc[idx]['image']\n",
    "        img_path = os.path.join(self.root_dir, img_name)\n",
    "\n",
    "        # Load image\n",
    "        try:\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {img_path}: {e}\")\n",
    "            # Return a placeholder image in case of error\n",
    "            image = Image.new('RGB', (224, 224))\n",
    "\n",
    "        # Apply transformations if any\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Process caption: convert to indices\n",
    "        caption_tokens = [self.vocab.stoi[\"<SOS>\"]]  # Start with SOS token\n",
    "        caption_tokens.extend(self.vocab.numericalize(caption))\n",
    "        caption_tokens.append(self.vocab.stoi[\"<EOS>\"])  # End with EOS token\n",
    "\n",
    "        # Convert to tensor\n",
    "        caption_tensor = torch.tensor(caption_tokens)\n",
    "\n",
    "        return image, caption_tensor\n",
    "\n",
    "class FlickrCollate:\n",
    "    \"\"\"Custom collate function to handle variable-length captions\"\"\"\n",
    "    def __init__(self, pad_idx):\n",
    "        self.pad_idx = pad_idx\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            batch: List of tuples (image, caption)\n",
    "\n",
    "        Returns:\n",
    "            images: Tensor of shape (batch_size, 3, height, width)\n",
    "            captions: Padded tensor of shape (batch_size, max_length)\n",
    "            caption_lengths: List of caption lengths\n",
    "        \"\"\"\n",
    "        # Sort batch by caption length (descending) for packing\n",
    "        batch.sort(key=lambda x: len(x[1]), reverse=True)\n",
    "\n",
    "        # Separate images and captions\n",
    "        images, captions = zip(*batch)\n",
    "\n",
    "        # Stack images\n",
    "        images = torch.stack(images, dim=0)\n",
    "\n",
    "        # Get caption lengths\n",
    "        caption_lengths = [len(cap) for cap in captions]\n",
    "\n",
    "        # Pad captions to have same length\n",
    "        captions_padded = pad_sequence(captions, batch_first=True, padding_value=self.pad_idx)\n",
    "\n",
    "        return images, captions_padded, caption_lengths\n",
    "\n",
    "# Create dataset objects\n",
    "train_dataset = FlickrDataset(\n",
    "    data_df=train_df,\n",
    "    root_dir=IMAGES_DIR,\n",
    "    vocab=vocab,\n",
    "    transform=transform_train\n",
    ")\n",
    "\n",
    "val_dataset = FlickrDataset(\n",
    "    data_df=val_df,\n",
    "    root_dir=IMAGES_DIR,\n",
    "    vocab=vocab,\n",
    "    transform=transform_val\n",
    ")\n",
    "\n",
    "test_dataset = FlickrDataset(\n",
    "    data_df=test_df,\n",
    "    root_dir=IMAGES_DIR,\n",
    "    vocab=vocab,\n",
    "    transform=transform_val\n",
    ")\n",
    "\n",
    "print(\"\\nDataset sizes:\")\n",
    "print(f\"Train: {len(train_dataset)} samples\")\n",
    "print(f\"Validation: {len(val_dataset)} samples\")\n",
    "print(f\"Test: {len(test_dataset)} samples\")\n",
    "\n",
    "# Create DataLoaders\n",
    "def create_data_loaders(train_dataset, val_dataset, test_dataset, batch_size, vocab):\n",
    "    \"\"\"Create data loaders for all splits\"\"\"\n",
    "    pad_idx = vocab.stoi[\"<PAD>\"]\n",
    "    \n",
    "    # Worker settings (use 0 for Windows compatibility if needed)\n",
    "    num_workers = 4 if sys.platform != 'win32' else 0\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        collate_fn=FlickrCollate(pad_idx=pad_idx),\n",
    "        pin_memory=torch.cuda.is_available()\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        dataset=val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        collate_fn=FlickrCollate(pad_idx=pad_idx),\n",
    "        pin_memory=torch.cuda.is_available()\n",
    "    )\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        dataset=test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        collate_fn=FlickrCollate(pad_idx=pad_idx),\n",
    "        pin_memory=torch.cuda.is_available()\n",
    "    )\n",
    "    \n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "# Create dataloaders\n",
    "batch_size = model_config['batch_size']\n",
    "train_loader, val_loader, test_loader = create_data_loaders(\n",
    "    train_dataset, val_dataset, test_dataset, batch_size, vocab\n",
    ")\n",
    "\n",
    "print(f\"\\nDataLoader batches:\")\n",
    "print(f\"Train: {len(train_loader)} batches\")\n",
    "print(f\"Validation: {len(val_loader)} batches\")\n",
    "print(f\"Test: {len(test_loader)} batches\")\n",
    "\n",
    "# Create smaller debug loaders\n",
    "def create_debug_loader(dataset, batch_size=8, num_samples=100):\n",
    "    \"\"\"Create a smaller loader for debugging\"\"\"\n",
    "    indices = list(range(min(num_samples, len(dataset))))\n",
    "    subset = Subset(dataset, indices)\n",
    "    \n",
    "    pad_idx = vocab.stoi[\"<PAD>\"]\n",
    "    \n",
    "    loader = DataLoader(\n",
    "        dataset=subset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=0,  # Use single process\n",
    "        collate_fn=FlickrCollate(pad_idx=pad_idx),\n",
    "        pin_memory=False\n",
    "    )\n",
    "    \n",
    "    return loader\n",
    "\n",
    "# Create debug loaders if in debug mode\n",
    "if DEBUG_MODE:\n",
    "    debug_train_loader = create_debug_loader(train_dataset, batch_size=8, num_samples=100)\n",
    "    debug_val_loader = create_debug_loader(val_dataset, batch_size=8, num_samples=50)\n",
    "    debug_test_loader = create_debug_loader(test_dataset, batch_size=8, num_samples=50)\n",
    "    \n",
    "    print(\"\\nDebug loader sizes:\")\n",
    "    print(f\"Debug train: {len(debug_train_loader)} batches\")\n",
    "    print(f\"Debug val: {len(debug_val_loader)} batches\")\n",
    "    print(f\"Debug test: {len(debug_test_loader)} batches\")\n",
    "    \n",
    "    # Use debug loaders for training/validation\n",
    "    train_loader_to_use = debug_train_loader\n",
    "    val_loader_to_use = debug_val_loader\n",
    "    test_loader_to_use = debug_test_loader\n",
    "else:\n",
    "    # Use full loaders for training/validation\n",
    "    train_loader_to_use = train_loader\n",
    "    val_loader_to_use = val_loader\n",
    "    test_loader_to_use = test_loader\n",
    "\n",
    "# Sample batch inspection\n",
    "def inspect_batch(data_loader, vocab):\n",
    "    \"\"\"Inspect a batch to verify dataloader works correctly\"\"\"\n",
    "    # Get a batch\n",
    "    images, captions, lengths = next(iter(data_loader))\n",
    "\n",
    "    print(f\"Batch contents:\")\n",
    "    print(f\"Images shape: {images.shape}\")\n",
    "    print(f\"Captions shape: {captions.shape}\")\n",
    "    print(f\"Caption lengths: {lengths[:5]}...\")\n",
    "\n",
    "    # Display one sample\n",
    "    idx = 0  # First sample in batch\n",
    "    caption = captions[idx]\n",
    "    caption_words = [vocab.itos[token_idx.item()] for token_idx in caption \n",
    "                    if token_idx.item() < len(vocab)]\n",
    "    \n",
    "    print(f\"\\nSample caption: {' '.join(caption_words)}\")\n",
    "    \n",
    "    # Convert image for display\n",
    "    img = images[idx].permute(1, 2, 0).numpy()\n",
    "    img = img * np.array(IMAGENET_STD) + np.array(IMAGENET_MEAN)  # Denormalize\n",
    "    img = np.clip(img, 0, 1)\n",
    "    \n",
    "    # Display image\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(img)\n",
    "    plt.title(\"Sample Image\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    return images, captions, lengths\n",
    "\n",
    "# Save vocabulary to disk\n",
    "with open(os.path.join(OUTPUT_DIR, 'vocabulary.pkl'), 'wb') as f:\n",
    "    pickle.dump(vocab, f)\n",
    "print(f\"\\nVocabulary saved to {os.path.join(OUTPUT_DIR, 'vocabulary.pkl')}\")\n",
    "\n",
    "# Inspect a batch from train loader\n",
    "print(\"\\nInspecting a batch from the training loader:\")\n",
    "_ = inspect_batch(train_loader_to_use, vocab)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a000d8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model Architecture\n",
    "\n",
    "### 3.1 Encoder Component\n",
    "class EncoderCNN(nn.Module):\n",
    "    \"\"\"CNN encoder for extracting image features\"\"\"\n",
    "\n",
    "    def __init__(self, embed_size, dropout=0.5, train_cnn=False, attention=False):\n",
    "        super(EncoderCNN, self).__init__()\n",
    "\n",
    "        # Load pre-trained ResNet-50\n",
    "        resnet = models.resnet50(pretrained=True)\n",
    "\n",
    "        # Different handling for attention vs baseline\n",
    "        if attention:\n",
    "            # For attention: Keep spatial information, remove final FC and pooling\n",
    "            modules = list(resnet.children())[:-2]\n",
    "            print(f\"Initializing Encoder CNN with spatial features:\")\n",
    "            self.feature_size = 2048  # ResNet features without pooling\n",
    "            \n",
    "            # Conv layer to reduce channel dimension\n",
    "            self.conv = nn.Conv2d(self.feature_size, embed_size, kernel_size=1)\n",
    "            \n",
    "        else:\n",
    "            # For baseline: Use pooled features\n",
    "            modules = list(resnet.children())[:-1]\n",
    "            print(f\"Initializing Encoder CNN:\")\n",
    "            # Save the feature size\n",
    "            self.feature_size = resnet.fc.in_features\n",
    "            \n",
    "            # Project to embedding space\n",
    "            self.fc = nn.Linear(self.feature_size, embed_size)\n",
    "            \n",
    "            # Use LayerNorm instead of BatchNorm (works with small batches)\n",
    "            self.norm = nn.LayerNorm(embed_size)\n",
    "            \n",
    "        # Create resnet feature extractor\n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "        self.attention = attention\n",
    "\n",
    "        # Freeze or unfreeze CNN\n",
    "        for param in self.resnet.parameters():\n",
    "            param.requires_grad = train_cnn\n",
    "\n",
    "        # Additional layers\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Print architecture info\n",
    "        print(f\"  Embed size: {embed_size}\")\n",
    "        print(f\"  Feature size: {self.feature_size}\")\n",
    "        print(f\"  Using attention: {attention}\")\n",
    "        print(f\"  Training CNN backbone: {train_cnn}\")\n",
    "        \n",
    "        # Count parameters\n",
    "        if attention:\n",
    "            print(f\"  Conv parameters: {sum(p.numel() for p in self.conv.parameters()):,}\")\n",
    "        else:\n",
    "            print(f\"  FC parameters: {sum(p.numel() for p in self.fc.parameters()):,}\")\n",
    "        print(f\"  Total parameters: {sum(p.numel() for p in self.parameters()):,}\")\n",
    "        print(f\"  Trainable parameters: {sum(p.numel() for p in self.parameters() if p.requires_grad):,}\")\n",
    "\n",
    "    def forward(self, images, debug=False):\n",
    "        \"\"\"Extract features from images\"\"\"\n",
    "        if debug:\n",
    "            debug_print(\"Encoder input\", images)\n",
    "\n",
    "        # Get features from ResNet\n",
    "        features = self.resnet(images)\n",
    "\n",
    "        if debug:\n",
    "            debug_print(\"ResNet output\", features)\n",
    "\n",
    "        # Different processing for attention vs baseline\n",
    "        if self.attention:\n",
    "            # For attention model: Use 1x1 conv to reduce channels\n",
    "            features = self.conv(features)\n",
    "            \n",
    "            if debug:\n",
    "                debug_print(\"After 1x1 conv\", features)\n",
    "                \n",
    "            # Apply ReLU and dropout\n",
    "            features = self.dropout(self.relu(features))\n",
    "                \n",
    "        else:\n",
    "            # For baseline model: Flatten and project\n",
    "            # Reshape: (batch_size, 2048, 1, 1) -> (batch_size, 2048)\n",
    "            features = features.view(features.size(0), -1)\n",
    "            \n",
    "            if debug:\n",
    "                debug_print(\"Reshaped features\", features)\n",
    "            \n",
    "            # Project to embedding space\n",
    "            features = self.fc(features)\n",
    "            \n",
    "            if debug:\n",
    "                debug_print(\"After FC projection\", features)\n",
    "            \n",
    "            # Apply normalization, dropout and ReLU\n",
    "            features = self.norm(features)\n",
    "            \n",
    "            if debug:\n",
    "                debug_print(\"After normalization\", features)\n",
    "                \n",
    "            features = self.dropout(self.relu(features))\n",
    "\n",
    "        if debug:\n",
    "            debug_print(\"Final encoder output\", features)\n",
    "\n",
    "        return features\n",
    "\n",
    "### 3.2 Attention Mechanism\n",
    "class Attention(nn.Module):\n",
    "    \"\"\"Attention mechanism for focusing on specific parts of the image\"\"\"\n",
    "\n",
    "    def __init__(self, encoder_dim, decoder_dim, attention_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        \n",
    "        # Layers for attention mechanism\n",
    "        self.encoder_att = nn.Linear(encoder_dim, attention_dim)\n",
    "        self.decoder_att = nn.Linear(decoder_dim, attention_dim)\n",
    "        self.full_att = nn.Linear(attention_dim, 1)\n",
    "        \n",
    "        # Activation functions\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "        # Print architecture info\n",
    "        print(f\"Initializing Attention mechanism:\")\n",
    "        print(f\"  Encoder dimension: {encoder_dim}\")\n",
    "        print(f\"  Decoder dimension: {decoder_dim}\")\n",
    "        print(f\"  Attention dimension: {attention_dim}\")\n",
    "        \n",
    "        # Count parameters\n",
    "        print(f\"  Attention parameters: {sum(p.numel() for p in self.parameters()):,}\")\n",
    "\n",
    "    def forward(self, encoder_out, decoder_hidden):\n",
    "        \"\"\"\n",
    "        Forward pass of the attention layer\n",
    "        \n",
    "        Args:\n",
    "            encoder_out: Feature maps from encoder, shape (batch_size, num_pixels, encoder_dim)\n",
    "            decoder_hidden: Hidden state of the decoder, shape (batch_size, decoder_dim)\n",
    "            \n",
    "        Returns:\n",
    "            attention_weighted_encoding: Weighted sum of encoder outputs (batch_size, encoder_dim)\n",
    "            alpha: Attention weights (batch_size, num_pixels)\n",
    "        \"\"\"\n",
    "        # Transform encoder output for attention\n",
    "        att1 = self.encoder_att(encoder_out)  # (batch_size, num_pixels, attention_dim)\n",
    "        \n",
    "        # Transform decoder hidden state for attention\n",
    "        att2 = self.decoder_att(decoder_hidden)  # (batch_size, attention_dim)\n",
    "        \n",
    "        # Sum and apply non-linearity\n",
    "        att = self.relu(att1 + att2.unsqueeze(1))  # (batch_size, num_pixels, attention_dim)\n",
    "        \n",
    "        # Compute attention scores\n",
    "        att = self.full_att(att).squeeze(2)  # (batch_size, num_pixels)\n",
    "        \n",
    "        # Apply softmax to get attention weights\n",
    "        alpha = self.softmax(att)  # (batch_size, num_pixels)\n",
    "        \n",
    "        # Compute weighted encoding\n",
    "        attention_weighted_encoding = (encoder_out * alpha.unsqueeze(2)).sum(dim=1)  # (batch_size, encoder_dim)\n",
    "        \n",
    "        return attention_weighted_encoding, alpha\n",
    "\n",
    "### 3.3 Decoder Component\n",
    "class DecoderRNN(nn.Module):\n",
    "    \"\"\"RNN decoder for generating captions\"\"\"\n",
    "\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers=1, dropout=0.5):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "\n",
    "        # Store parameters\n",
    "        self.embed_size = embed_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Print architecture information\n",
    "        print(f\"\\nInitializing Decoder RNN:\")\n",
    "        print(f\"  Embed size: {embed_size}\")\n",
    "        print(f\"  Hidden size: {hidden_size}\")\n",
    "        print(f\"  Vocabulary size: {vocab_size}\")\n",
    "        print(f\"  LSTM layers: {num_layers}\")\n",
    "\n",
    "        # Word embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "\n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embed_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "\n",
    "        # Output layer\n",
    "        self.output_layer = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Print parameter counts\n",
    "        embed_params = sum(p.numel() for p in self.embedding.parameters())\n",
    "        lstm_params = sum(p.numel() for p in self.lstm.parameters())\n",
    "        output_params = sum(p.numel() for p in self.output_layer.parameters())\n",
    "\n",
    "        print(f\"  Embedding parameters: {embed_params:,}\")\n",
    "        print(f\"  LSTM parameters: {lstm_params:,}\")\n",
    "        print(f\"  Output layer parameters: {output_params:,}\")\n",
    "        print(f\"  Total parameters: {sum(p.numel() for p in self.parameters()):,}\")\n",
    "\n",
    "    def forward_with_teacher_forcing(self, features, captions, caption_lengths, debug=False):\n",
    "        \"\"\"\n",
    "        Forward pass with teacher forcing during training.\n",
    "\n",
    "        Args:\n",
    "            features: Image features from encoder (batch_size, embed_size)\n",
    "            captions: Ground truth captions (batch_size, max_length)\n",
    "            caption_lengths: True lengths of each caption\n",
    "            debug: Whether to print debug info\n",
    "\n",
    "        Returns:\n",
    "            outputs: Predicted word scores (batch_size, max_length, vocab_size)\n",
    "        \"\"\"\n",
    "        batch_size = features.size(0)\n",
    "        max_length = captions.size(1)\n",
    "\n",
    "        if debug:\n",
    "            debug_print(\"Decoder input\", features, level=1)\n",
    "            debug_print(f\"Caption shape: {captions.shape}, Lengths: {caption_lengths}\", None, level=1)\n",
    "\n",
    "        # Prepare embeddings for captions\n",
    "        embeddings = self.dropout(self.embedding(captions))\n",
    "\n",
    "        if debug:\n",
    "            debug_print(\"Caption embeddings\", embeddings, level=1)\n",
    "\n",
    "        # Prepare to include features as first input\n",
    "        # Reshape features: (batch_size, embed_size) -> (batch_size, 1, embed_size)\n",
    "        features = features.unsqueeze(1)\n",
    "\n",
    "        # For teacher forcing, we'll use features for the first time step,\n",
    "        # then the embeddings of the ground truth tokens\n",
    "        decoder_input = torch.cat([features, embeddings[:, :-1, :]], dim=1)\n",
    "\n",
    "        if debug:\n",
    "            debug_print(\"Decoder input sequence\", decoder_input, level=1)\n",
    "\n",
    "        # Run through LSTM\n",
    "        outputs, _ = self.lstm(decoder_input)\n",
    "\n",
    "        if debug:\n",
    "            debug_print(\"LSTM outputs\", outputs, level=1)\n",
    "\n",
    "        # Generate word scores\n",
    "        outputs = self.output_layer(outputs)\n",
    "\n",
    "        if debug:\n",
    "            debug_print(\"Final outputs\", outputs, level=1)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def sample(self, features, max_length=20, debug=False):\n",
    "        \"\"\"\n",
    "        Generate captions using greedy search.\n",
    "\n",
    "        Args:\n",
    "            features: Image features from encoder (batch_size, embed_size)\n",
    "            max_length: Maximum caption length\n",
    "            debug: Whether to print debug info\n",
    "\n",
    "        Returns:\n",
    "            sampled_ids: Predicted caption indices (batch_size, max_length)\n",
    "        \"\"\"\n",
    "        batch_size = features.size(0)\n",
    "\n",
    "        if debug:\n",
    "            debug_print(\"Sampling - Feature input\", features, level=1)\n",
    "\n",
    "        # Initialize result tensor\n",
    "        sampled_ids = torch.zeros(batch_size, max_length, dtype=torch.long, device=features.device)\n",
    "\n",
    "        # Initialize hidden and cell states\n",
    "        h = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(features.device)\n",
    "        c = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(features.device)\n",
    "\n",
    "        # First input is the image features\n",
    "        input_word = features\n",
    "\n",
    "        # Generate words one by one\n",
    "        for i in range(max_length):\n",
    "            if debug and i == 0:\n",
    "                debug_print(f\"Sampling step {i} - Input\", input_word, level=1)\n",
    "\n",
    "            # Run LSTM step\n",
    "            output, (h, c) = self.lstm(input_word.unsqueeze(1), (h, c))\n",
    "\n",
    "            if debug and i == 0:\n",
    "                debug_print(f\"Sampling step {i} - LSTM output\", output, level=1)\n",
    "\n",
    "            # Get word predictions\n",
    "            output = self.output_layer(output.squeeze(1))\n",
    "\n",
    "            if debug and i == 0:\n",
    "                debug_print(f\"Sampling step {i} - Word scores\", output, level=1)\n",
    "\n",
    "            # Greedy search - pick highest probability word\n",
    "            predicted = output.argmax(dim=1)\n",
    "\n",
    "            if debug and i == 0:\n",
    "                debug_print(f\"Sampling step {i} - Predicted word indices: {predicted}\", None, level=1)\n",
    "\n",
    "            # Save prediction\n",
    "            sampled_ids[:, i] = predicted\n",
    "\n",
    "            # Next input is the predicted word embedding\n",
    "            input_word = self.embedding(predicted)\n",
    "\n",
    "        if debug:\n",
    "            debug_print(f\"Sampling complete - Output shape: {sampled_ids.shape}\", level=1)\n",
    "\n",
    "        return sampled_ids\n",
    "\n",
    "### 3.4 Attention Decoder Component\n",
    "class AttentionDecoderRNN(nn.Module):\n",
    "    \"\"\"RNN decoder that uses attention mechanism for generating captions\"\"\"\n",
    "\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, encoder_dim, attention_dim, num_layers=1, dropout=0.5):\n",
    "        super(AttentionDecoderRNN, self).__init__()\n",
    "        \n",
    "        # Store parameters\n",
    "        self.embed_size = embed_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.encoder_dim = encoder_dim\n",
    "        self.attention_dim = attention_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Print architecture information\n",
    "        print(f\"\\nInitializing Attention Decoder RNN:\")\n",
    "        print(f\"  Embed size: {embed_size}\")\n",
    "        print(f\"  Hidden size: {hidden_size}\")\n",
    "        print(f\"  Vocabulary size: {vocab_size}\")\n",
    "        print(f\"  Encoder dimension: {encoder_dim}\")\n",
    "        print(f\"  Attention dimension: {attention_dim}\")\n",
    "        print(f\"  LSTM layers: {num_layers}\")\n",
    "        \n",
    "        # Word embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        \n",
    "        # Attention mechanism\n",
    "        self.attention = Attention(encoder_dim, hidden_size, attention_dim)\n",
    "        \n",
    "        # Decoder LSTM\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embed_size + encoder_dim,  # Input is concat of embedding and context\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "        \n",
    "        # Layer to compute initial hidden/cell states from mean of encoder output\n",
    "        self.init_h = nn.Linear(encoder_dim, hidden_size)\n",
    "        self.init_c = nn.Linear(encoder_dim, hidden_size)\n",
    "        \n",
    "        # Layer to produce word scores\n",
    "        self.fc = nn.Linear(hidden_size + encoder_dim, vocab_size)\n",
    "        \n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Print parameter counts\n",
    "        embed_params = sum(p.numel() for p in self.embedding.parameters())\n",
    "        att_params = sum(p.numel() for p in self.attention.parameters())\n",
    "        lstm_params = sum(p.numel() for p in self.lstm.parameters())\n",
    "        fc_params = sum(p.numel() for p in self.fc.parameters())\n",
    "        \n",
    "        print(f\"  Embedding parameters: {embed_params:,}\")\n",
    "        print(f\"  Attention parameters: {att_params:,}\")\n",
    "        print(f\"  LSTM parameters: {lstm_params:,}\")\n",
    "        print(f\"  Output layer parameters: {fc_params:,}\")\n",
    "        print(f\"  Total parameters: {sum(p.numel() for p in self.parameters()):,}\")\n",
    "\n",
    "    def init_hidden_state(self, encoder_out):\n",
    "        \"\"\"\n",
    "        Initialize LSTM hidden and cell states using the encoder output\n",
    "        \n",
    "        Args:\n",
    "            encoder_out: Feature maps from encoder, shape (batch_size, num_pixels, encoder_dim)\n",
    "            \n",
    "        Returns:\n",
    "            h, c: Initial hidden and cell states\n",
    "        \"\"\"\n",
    "        # Mean of encoder output across all pixels\n",
    "        mean_encoder_out = encoder_out.mean(dim=1)  # (batch_size, encoder_dim)\n",
    "        \n",
    "        # Project to get initial states\n",
    "        h = self.init_h(mean_encoder_out)  # (batch_size, hidden_size)\n",
    "        c = self.init_c(mean_encoder_out)  # (batch_size, hidden_size)\n",
    "        \n",
    "        # Reshape for LSTM which expects (num_layers, batch_size, hidden_size)\n",
    "        h = h.unsqueeze(0).repeat(self.num_layers, 1, 1)\n",
    "        c = c.unsqueeze(0).repeat(self.num_layers, 1, 1)\n",
    "        \n",
    "        return h, c\n",
    "        \n",
    "    def forward_with_teacher_forcing(self, encoder_out, captions, caption_lengths, debug=False):\n",
    "        \"\"\"\n",
    "        Forward pass with teacher forcing during training\n",
    "        \n",
    "        Args:\n",
    "            encoder_out: Feature maps from encoder, shape (batch_size, encoder_dim, height, width)\n",
    "            captions: Ground truth captions (batch_size, max_length)\n",
    "            caption_lengths: True lengths of each caption\n",
    "            debug: Whether to print debug info\n",
    "            \n",
    "        Returns:\n",
    "            outputs: Predicted word scores (batch_size, max_length, vocab_size)\n",
    "            alphas: Attention weights for visualization (batch_size, max_length, num_pixels)\n",
    "        \"\"\"\n",
    "        batch_size = encoder_out.size(0)\n",
    "        encoder_dim = encoder_out.size(1)\n",
    "        height = encoder_out.size(2)\n",
    "        width = encoder_out.size(3)\n",
    "        max_length = captions.size(1)\n",
    "        \n",
    "        if debug:\n",
    "            debug_print(\"Decoder input - encoder out\", encoder_out, level=1)\n",
    "            debug_print(f\"Caption shape: {captions.shape}, Lengths: {caption_lengths}\", None, level=1)\n",
    "        \n",
    "        # Flatten spatial dimensions of encoder output for attention\n",
    "        encoder_out = encoder_out.permute(0, 2, 3, 1)  # (batch_size, height, width, encoder_dim)\n",
    "        encoder_out = encoder_out.view(batch_size, -1, encoder_dim)  # (batch_size, num_pixels, encoder_dim)\n",
    "        num_pixels = encoder_out.size(1)\n",
    "        \n",
    "        if debug:\n",
    "            debug_print(\"Flattened encoder out\", encoder_out, level=1)\n",
    "        \n",
    "        # Initialize LSTM hidden and cell states\n",
    "        h, c = self.init_hidden_state(encoder_out)\n",
    "        \n",
    "        # Prepare embeddings for captions\n",
    "        embeddings = self.dropout(self.embedding(captions))  # (batch_size, max_length, embed_size)\n",
    "        \n",
    "        if debug:\n",
    "            debug_print(\"Caption embeddings\", embeddings, level=1)\n",
    "        \n",
    "        # Initialize tensors for predictions and attention weights\n",
    "        predictions = torch.zeros(batch_size, max_length, self.vocab_size).to(captions.device)\n",
    "        alphas = torch.zeros(batch_size, max_length, num_pixels).to(captions.device)\n",
    "        \n",
    "        # For each time step\n",
    "        for t in range(max_length):\n",
    "            # Get hidden state for attention (using last layer's hidden state)\n",
    "            h_for_att = h[-1]  # (batch_size, hidden_size)\n",
    "            \n",
    "            # Compute attention\n",
    "            context, alpha = self.attention(encoder_out, h_for_att)\n",
    "            \n",
    "            if debug and t == 0:\n",
    "                debug_print(f\"Time step {t} - Context\", context, level=1)\n",
    "                debug_print(f\"Time step {t} - Attention weights shape: {alpha.shape}\", None, level=1)\n",
    "            \n",
    "            # Store attention weights\n",
    "            alphas[:, t] = alpha\n",
    "            \n",
    "            # Prepare input for LSTM - concatenate context with embedding\n",
    "            lstm_input = torch.cat([embeddings[:, t], context], dim=1).unsqueeze(1)\n",
    "            \n",
    "            # Run LSTM step\n",
    "            output, (h, c) = self.lstm(lstm_input, (h, c))\n",
    "            \n",
    "            # Reshape output\n",
    "            output = output.squeeze(1)  # (batch_size, hidden_size)\n",
    "            \n",
    "            # Concatenate output with context for final prediction\n",
    "            output = torch.cat([output, context], dim=1)  # (batch_size, hidden_size + encoder_dim)\n",
    "            \n",
    "            # Generate word scores\n",
    "            preds = self.fc(self.dropout(output))  # (batch_size, vocab_size)\n",
    "            \n",
    "            # Store predictions\n",
    "            predictions[:, t] = preds\n",
    "        \n",
    "        if debug:\n",
    "            debug_print(\"Final predictions\", predictions, level=1)\n",
    "            debug_print(\"Attention weights\", alphas, level=1)\n",
    "        \n",
    "        return predictions, alphas\n",
    "    \n",
    "    def sample(self, encoder_out, max_length=20, debug=False):\n",
    "        \"\"\"\n",
    "        Generate captions using attention and greedy search\n",
    "        \n",
    "        Args:\n",
    "            encoder_out: Feature maps from encoder (batch_size, encoder_dim, height, width)\n",
    "            max_length: Maximum caption length\n",
    "            debug: Whether to print debug info\n",
    "            \n",
    "        Returns:\n",
    "            sampled_ids: Predicted caption indices (batch_size, max_length)\n",
    "            alphas: Attention weights for visualization (batch_size, max_length, num_pixels)\n",
    "        \"\"\"\n",
    "        batch_size = encoder_out.size(0)\n",
    "        encoder_dim = encoder_out.size(1)\n",
    "        height = encoder_out.size(2)\n",
    "        width = encoder_out.size(3)\n",
    "        \n",
    "        if debug:\n",
    "            debug_print(\"Sampling - Feature input\", encoder_out, level=1)\n",
    "        \n",
    "        # Flatten spatial dimensions for attention\n",
    "        encoder_out = encoder_out.permute(0, 2, 3, 1)  # (batch_size, height, width, encoder_dim)\n",
    "        encoder_out = encoder_out.view(batch_size, -1, encoder_dim)  # (batch_size, num_pixels, encoder_dim)\n",
    "        num_pixels = encoder_out.size(1)\n",
    "        \n",
    "        # Initialize LSTM hidden and cell states\n",
    "        h, c = self.init_hidden_state(encoder_out)\n",
    "        \n",
    "        # Initialize result tensors\n",
    "        sampled_ids = torch.zeros(batch_size, max_length, dtype=torch.long).to(encoder_out.device)\n",
    "        alphas = torch.zeros(batch_size, max_length, num_pixels).to(encoder_out.device)\n",
    "        \n",
    "        # Start with <SOS> token (index 1)\n",
    "        input_word = torch.ones(batch_size, dtype=torch.long).to(encoder_out.device)\n",
    "        \n",
    "        # Generate words one by one\n",
    "        for t in range(max_length):\n",
    "            # Embed the input word\n",
    "            embedded = self.embedding(input_word)  # (batch_size, embed_size)\n",
    "            \n",
    "            # Get hidden state for attention\n",
    "            h_for_att = h[-1]  # (batch_size, hidden_size)\n",
    "            \n",
    "            # Compute attention\n",
    "            context, alpha = self.attention(encoder_out, h_for_att)\n",
    "            \n",
    "            # Store attention weights\n",
    "            alphas[:, t] = alpha\n",
    "            \n",
    "            # Prepare input for LSTM\n",
    "            lstm_input = torch.cat([embedded, context], dim=1).unsqueeze(1)  # (batch_size, 1, embed_size + encoder_dim)\n",
    "            \n",
    "            # Run LSTM step\n",
    "            output, (h, c) = self.lstm(lstm_input, (h, c))\n",
    "            \n",
    "            # Reshape output\n",
    "            output = output.squeeze(1)  # (batch_size, hidden_size)\n",
    "            \n",
    "            # Concatenate output with context\n",
    "            output = torch.cat([output, context], dim=1)  # (batch_size, hidden_size + encoder_dim)\n",
    "            \n",
    "            # Generate word scores\n",
    "            preds = self.fc(self.dropout(output))  # (batch_size, vocab_size)\n",
    "            \n",
    "            # Greedy search - pick highest probability word\n",
    "            predicted = preds.argmax(dim=1)  # (batch_size)\n",
    "            \n",
    "            # Store prediction\n",
    "            sampled_ids[:, t] = predicted\n",
    "            \n",
    "            # Next input is the predicted word\n",
    "            input_word = predicted\n",
    "        \n",
    "        if debug:\n",
    "            debug_print(f\"Sampling complete - Output shape: {sampled_ids.shape}\", level=1)\n",
    "            debug_print(f\"Attention weights shape: {alphas.shape}\", level=1)\n",
    "        \n",
    "        return sampled_ids, alphas\n",
    "\n",
    "### 3.5 Baseline Caption Model\n",
    "class BaselineCaptionModel(nn.Module):\n",
    "    \"\"\"Complete CNN-RNN model for image captioning\"\"\"\n",
    "\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers=1, dropout=0.5):\n",
    "        super(BaselineCaptionModel, self).__init__()\n",
    "\n",
    "        # Print architecture information\n",
    "        print(\"\\nInitializing Baseline Caption Model\")\n",
    "        print(f\"  Embed size: {embed_size}\")\n",
    "        print(f\"  Hidden size: {hidden_size}\")\n",
    "        print(f\"  Vocabulary size: {vocab_size}\")\n",
    "        print(f\"  LSTM layers: {num_layers}\")\n",
    "\n",
    "        # Create encoder and decoder\n",
    "        self.encoder = EncoderCNN(embed_size, dropout, attention=False)\n",
    "        self.decoder = DecoderRNN(embed_size, hidden_size, vocab_size, num_layers, dropout)\n",
    "\n",
    "        # Print total parameters\n",
    "        total_params = sum(p.numel() for p in self.parameters())\n",
    "        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "\n",
    "        print(f\"\\nModel Summary:\")\n",
    "        print(f\"  Total parameters: {total_params:,}\")\n",
    "        print(f\"  Trainable parameters: {trainable_params:,}\")\n",
    "\n",
    "    def forward(self, images, captions, caption_lengths, debug=False):\n",
    "        \"\"\"\n",
    "        Forward pass for training.\n",
    "\n",
    "        Args:\n",
    "            images: Input images (batch_size, 3, height, width)\n",
    "            captions: Caption indices (batch_size, max_length)\n",
    "            caption_lengths: True lengths of captions\n",
    "            debug: Whether to print debug info\n",
    "\n",
    "        Returns:\n",
    "            outputs: Predicted word scores\n",
    "        \"\"\"\n",
    "        if debug:\n",
    "            start_time = time.time()\n",
    "            debug_print(\"Starting forward pass\", None, level=0)\n",
    "            debug_print(\"Input shapes - Images: {}, Captions: {}\".format(\n",
    "                images.shape, captions.shape), None, level=0)\n",
    "\n",
    "        # Extract image features\n",
    "        features = self.encoder(images, debug)\n",
    "\n",
    "        # Generate captions\n",
    "        outputs = self.decoder.forward_with_teacher_forcing(\n",
    "            features, captions, caption_lengths, debug)\n",
    "\n",
    "        if debug:\n",
    "            debug_print(f\"Forward pass completed in {time.time() - start_time:.4f}s\", None, level=0)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def sample(self, images, max_length=20, debug=False):\n",
    "        \"\"\"\n",
    "        Generate captions for given images.\n",
    "\n",
    "        Args:\n",
    "            images: Input images (batch_size, 3, height, width)\n",
    "            max_length: Maximum caption length\n",
    "            debug: Whether to print debug info\n",
    "\n",
    "        Returns:\n",
    "            sampled_ids: Generated caption indices\n",
    "        \"\"\"\n",
    "        if debug:\n",
    "            start_time = time.time()\n",
    "            debug_print(\"Starting sampling\", None, level=0)\n",
    "\n",
    "        # Extract image features\n",
    "        features = self.encoder(images, debug)\n",
    "\n",
    "        # Generate captions\n",
    "        sampled_ids = self.decoder.sample(features, max_length, debug)\n",
    "\n",
    "        if debug:\n",
    "            debug_print(f\"Sampling completed in {time.time() - start_time:.4f}s\", None, level=0)\n",
    "\n",
    "        return sampled_ids\n",
    "\n",
    "    def caption_image(self, image, vocab, max_length=20, debug=False):\n",
    "        \"\"\"\n",
    "        Generate a caption for a single image.\n",
    "\n",
    "        Args:\n",
    "            image: Input image (1, 3, height, width)\n",
    "            vocab: Vocabulary object\n",
    "            max_length: Maximum caption length\n",
    "            debug: Whether to print debug info\n",
    "\n",
    "        Returns:\n",
    "            caption: Generated caption as string\n",
    "        \"\"\"\n",
    "        # Set to evaluation mode\n",
    "        self.eval()\n",
    "\n",
    "        if debug:\n",
    "            debug_print(\"Generating caption for image\", image, level=0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Generate caption indices\n",
    "            sampled_ids = self.sample(image, max_length, debug)\n",
    "\n",
    "            # Convert indices to words\n",
    "            sampled_ids = sampled_ids[0].cpu().numpy()\n",
    "\n",
    "            # Create caption\n",
    "            caption_words = []\n",
    "            for idx in sampled_ids:\n",
    "                word = vocab.itos[idx]\n",
    "\n",
    "                # Stop if EOS token\n",
    "                if word == \"<EOS>\":\n",
    "                    break\n",
    "\n",
    "                # Skip special tokens\n",
    "                if word not in [\"<PAD>\", \"<SOS>\"]:\n",
    "                    caption_words.append(word)\n",
    "\n",
    "            caption = \" \".join(caption_words)\n",
    "\n",
    "        if debug:\n",
    "            debug_print(f\"Generated caption: '{caption}'\", None, level=0)\n",
    "\n",
    "        return caption\n",
    "\n",
    "### 3.6 Attention Caption Model\n",
    "class AttentionCaptionModel(nn.Module):\n",
    "    \"\"\"Complete CNN-RNN model with attention for image captioning\"\"\"\n",
    "\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, attention_dim, num_layers=1, dropout=0.5):\n",
    "        super(AttentionCaptionModel, self).__init__()\n",
    "        \n",
    "        # Print architecture information\n",
    "        print(\"\\nInitializing Attention Caption Model\")\n",
    "        print(f\"  Embed size: {embed_size}\")\n",
    "        print(f\"  Hidden size: {hidden_size}\")\n",
    "        print(f\"  Vocabulary size: {vocab_size}\")\n",
    "        print(f\"  Attention dimension: {attention_dim}\")\n",
    "        print(f\"  LSTM layers: {num_layers}\")\n",
    "        \n",
    "        # Create encoder and decoder\n",
    "        self.encoder = EncoderCNN(embed_size, dropout, attention=True)\n",
    "        encoder_dim = embed_size  # The encoder projects to embed_size\n",
    "        \n",
    "        self.decoder = AttentionDecoderRNN(\n",
    "            embed_size, hidden_size, vocab_size, \n",
    "            encoder_dim, attention_dim, num_layers, dropout\n",
    "        )\n",
    "        \n",
    "        # Print total parameters\n",
    "        total_params = sum(p.numel() for p in self.parameters())\n",
    "        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "        \n",
    "        print(f\"\\nModel Summary:\")\n",
    "        print(f\"  Total parameters: {total_params:,}\")\n",
    "        print(f\"  Trainable parameters: {trainable_params:,}\")\n",
    "    \n",
    "    def forward(self, images, captions, caption_lengths, debug=False):\n",
    "        \"\"\"\n",
    "        Forward pass for training with teacher forcing\n",
    "        \n",
    "        Args:\n",
    "            images: Input images (batch_size, 3, height, width)\n",
    "            captions: Caption indices (batch_size, max_length)\n",
    "            caption_lengths: True lengths of captions\n",
    "            debug: Whether to print debug info\n",
    "            \n",
    "        Returns:\n",
    "            outputs: Predicted word scores\n",
    "            alphas: Attention weights\n",
    "        \"\"\"\n",
    "        if debug:\n",
    "            start_time = time.time()\n",
    "            debug_print(\"Starting forward pass\", None, level=0)\n",
    "            debug_print(\"Input shapes - Images: {}, Captions: {}\".format(\n",
    "                images.shape, captions.shape), None, level=0)\n",
    "        \n",
    "        # Extract image features\n",
    "        features = self.encoder(images, debug)\n",
    "        \n",
    "        # Generate captions with attention\n",
    "        outputs, alphas = self.decoder.forward_with_teacher_forcing(\n",
    "            features, captions, caption_lengths, debug)\n",
    "        \n",
    "        if debug:\n",
    "            debug_print(f\"Forward pass completed in {time.time() - start_time:.4f}s\", None, level=0)\n",
    "        \n",
    "        return outputs, alphas\n",
    "    \n",
    "    def sample(self, images, max_length=20, debug=False):\n",
    "        \"\"\"\n",
    "        Generate captions with attention for given images\n",
    "        \n",
    "        Args:\n",
    "            images: Input images (batch_size, 3, height, width)\n",
    "            max_length: Maximum caption length\n",
    "            debug: Whether to print debug info\n",
    "            \n",
    "        Returns:\n",
    "            sampled_ids: Generated caption indices\n",
    "            alphas: Attention weights for visualization\n",
    "        \"\"\"\n",
    "        if debug:\n",
    "            start_time = time.time()\n",
    "            debug_print(\"Starting sampling\", None, level=0)\n",
    "        \n",
    "        # Extract image features\n",
    "        features = self.encoder(images, debug)\n",
    "        \n",
    "        # Generate captions with attention\n",
    "        sampled_ids, alphas = self.decoder.sample(features, max_length, debug)\n",
    "        \n",
    "        if debug:\n",
    "            debug_print(f\"Sampling completed in {time.time() - start_time:.4f}s\", None, level=0)\n",
    "        \n",
    "        return sampled_ids, alphas\n",
    "    \n",
    "    def caption_image_with_attention(self, image, vocab, max_length=20, debug=False):\n",
    "        \"\"\"\n",
    "        Generate a caption with attention for a single image\n",
    "        \n",
    "        Args:\n",
    "            image: Input image (1, 3, height, width)\n",
    "            vocab: Vocabulary object\n",
    "            max_length: Maximum caption length\n",
    "            debug: Whether to print debug info\n",
    "            \n",
    "        Returns:\n",
    "            caption: Generated caption as string\n",
    "            alphas: Attention weights for visualization\n",
    "        \"\"\"\n",
    "        # Set to evaluation mode\n",
    "        self.eval()\n",
    "        \n",
    "        if debug:\n",
    "            debug_print(\"Generating caption with attention for image\", image, level=0)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Generate caption indices and attention weights\n",
    "            sampled_ids, alphas = self.sample(image, max_length, debug)\n",
    "            \n",
    "            # Convert indices to words\n",
    "            sampled_ids = sampled_ids[0].cpu().numpy()\n",
    "            \n",
    "            # Create caption\n",
    "            caption_words = []\n",
    "            attention_weights = []\n",
    "            \n",
    "            for i, idx in enumerate(sampled_ids):\n",
    "                word = vocab.itos[idx]\n",
    "                \n",
    "                # Stop if EOS token\n",
    "                if word == \"<EOS>\":\n",
    "                    attention_weights.append(alphas[0, i].cpu().numpy())\n",
    "                    break\n",
    "                \n",
    "                # Skip special tokens\n",
    "                if word not in [\"<PAD>\", \"<SOS>\"]:\n",
    "                    caption_words.append(word)\n",
    "                    attention_weights.append(alphas[0, i].cpu().numpy())\n",
    "            \n",
    "            caption = \" \".join(caption_words)\n",
    "        \n",
    "        if debug:\n",
    "            debug_print(f\"Generated caption: '{caption}'\", None, level=0)\n",
    "        \n",
    "        return caption, attention_weights\n",
    "\n",
    "### 3.7 Architecture Visualization Functions\n",
    "def visualize_model_architecture(model, is_attention_model=False):\n",
    "    \"\"\"Visualize model architecture with dimensions\"\"\"\n",
    "    import matplotlib.patches as patches\n",
    "\n",
    "    # Create figure\n",
    "    fig, ax = plt.subplots(figsize=(14, 10) if not is_attention_model else (15, 12))\n",
    "    ax.axis('off')\n",
    "\n",
    "    # Title\n",
    "    if is_attention_model:\n",
    "        ax.text(0.5, 0.97, \"Attention-based Image Captioning Architecture\", ha='center', fontsize=18, fontweight='bold')\n",
    "    else:\n",
    "        ax.text(0.5, 0.97, \"Baseline CNN+RNN Architecture\", ha='center', fontsize=18, fontweight='bold')\n",
    "\n",
    "    # Helper function to draw a box\n",
    "    def draw_box(x, y, width, height, label, details=None, color='lightblue'):\n",
    "        box = patches.Rectangle((x, y), width, height, fill=True, color=color, alpha=0.8,\n",
    "                            linewidth=2, edgecolor='black')\n",
    "        ax.add_patch(box)\n",
    "        ax.text(x + width/2, y + height/2, label, ha='center', va='center', fontsize=12, fontweight='bold')\n",
    "\n",
    "        if details:\n",
    "            ax.text(x + width + 0.02, y + height/2, details, ha='left', va='center', fontsize=10)\n",
    "\n",
    "        return box\n",
    "\n",
    "    # Dimensions for boxes\n",
    "    box_width = 0.6\n",
    "    box_height = 0.07\n",
    "    gap = 0.02\n",
    "    x = 0.2\n",
    "\n",
    "    if is_attention_model:\n",
    "        # =========== ATTENTION MODEL VISUALIZATION ===========\n",
    "        # Add section for encoder\n",
    "        y = 0.9\n",
    "        ax.text(0.5, 0.94, \"Encoder (Feature Extraction)\", fontsize=14, fontweight='bold', ha='center')\n",
    "        \n",
    "        # Input image\n",
    "        draw_box(x, y, box_width, box_height, \"Input Image\",\n",
    "                f\"Shape: (batch_size, 3, 224, 224)\", 'lightblue')\n",
    "        y -= box_height + gap\n",
    "        \n",
    "        # ResNet backbone\n",
    "        draw_box(x, y, box_width, box_height, \"ResNet-50 (up to layer4)\",\n",
    "                \"Pretrained CNN without pooling layer\", 'lightgreen')\n",
    "        y -= box_height + gap\n",
    "        \n",
    "        # Feature maps\n",
    "        feature_map_size = 224 // 32  # ResNet downsamples by factor of 32\n",
    "        draw_box(x, y, box_width, box_height, \"Feature Maps\",\n",
    "                f\"Shape: (batch_size, 2048, {feature_map_size}, {feature_map_size})\", 'lightyellow')\n",
    "        y -= box_height + gap\n",
    "        \n",
    "        # 1x1 convolution\n",
    "        draw_box(x, y, box_width, box_height, \"1x1 Convolution\",\n",
    "                f\"Shape: (batch_size, embed_size, {feature_map_size}, {feature_map_size})\", 'lightgreen')\n",
    "        y -= box_height + gap\n",
    "        \n",
    "        # Flattened features\n",
    "        draw_box(x, y, box_width, box_height, \"Flattened Features\",\n",
    "                f\"Shape: (batch_size, {feature_map_size*feature_map_size}, embed_size)\", 'lightyellow')\n",
    "        y -= box_height + 2*gap\n",
    "        \n",
    "        # Add section for attention\n",
    "        ax.text(0.5, y, \"Attention Mechanism\", fontsize=14, fontweight='bold', ha='center')\n",
    "        y -= gap\n",
    "        \n",
    "        # Attention mechanism\n",
    "        att_y = y\n",
    "        draw_box(x, y, box_width, box_height, \"Attention Weights\",\n",
    "                \"Shape: (batch_size, num_pixels)\", 'pink')\n",
    "        y -= box_height + gap\n",
    "        \n",
    "        # Weighted encoding\n",
    "        draw_box(x, y, box_width, box_height, \"Weighted Feature Vector\",\n",
    "                \"Shape: (batch_size, embed_size)\", 'lightyellow')\n",
    "        y -= box_height + 2*gap\n",
    "        \n",
    "        # Add section for decoder\n",
    "        ax.text(0.5, y, \"Decoder (Caption Generation)\", fontsize=14, fontweight='bold', ha='center')\n",
    "        y -= gap\n",
    "        \n",
    "        # Word embedding\n",
    "        draw_box(x, y, box_width, box_height, \"Word Embedding\",\n",
    "                \"Shape: (batch_size, embed_size)\", 'lightblue')\n",
    "        y -= box_height + gap\n",
    "        \n",
    "        # Combined input\n",
    "        draw_box(x, y, box_width, box_height, \"Combined Input\",\n",
    "                \"Shape: (batch_size, embed_size + encoder_dim)\", 'lightyellow')\n",
    "        y -= box_height + gap\n",
    "        \n",
    "        # LSTM\n",
    "        draw_box(x, y, box_width, box_height, \"LSTM\",\n",
    "                \"Hidden size, Layers, Dropout\", 'lightgreen')\n",
    "        y -= box_height + gap\n",
    "        \n",
    "        # Output projection\n",
    "        draw_box(x, y, box_width, box_height, \"Output Projection\",\n",
    "                \"Shape: (batch_size, vocab_size)\", 'lightgreen')\n",
    "        y -= box_height + gap\n",
    "        \n",
    "        # Word predictions\n",
    "        draw_box(x, y, box_width, box_height, \"Word Predictions\",\n",
    "                \"Cross-entropy loss during training\", 'lightyellow')\n",
    "        \n",
    "        # Add flow arrows\n",
    "        for i in range(10):  # Adjust based on the number of boxes\n",
    "            if i != 4 and i != 7:  # Skip arrows before each new section\n",
    "                arrow_y = 0.9 - i * (box_height + gap) - (0.5*gap if i >= 5 else 0) - (0.5*gap if i >= 8 else 0)\n",
    "                ax.annotate(\"\", xy=(x + box_width/2, arrow_y - box_height - gap),\n",
    "                         xytext=(x + box_width/2, arrow_y),\n",
    "                         arrowprops=dict(arrowstyle=\"->\", lw=2, color='black'))\n",
    "        \n",
    "        # Add arrows for attention flow\n",
    "        # From encoder to attention\n",
    "        att_x = x + box_width + 0.1\n",
    "        ax.annotate(\"\", xy=(att_x, att_y + box_height/2),\n",
    "                 xytext=(x + box_width, 0.9 - 4*(box_height + gap) - box_height/2),\n",
    "                 arrowprops=dict(arrowstyle=\"->\", lw=2, color='blue', connectionstyle=\"arc3,rad=0.3\"))\n",
    "        \n",
    "        # From decoder to attention\n",
    "        ax.annotate(\"\", xy=(att_x, att_y + box_height/2),\n",
    "                 xytext=(x + box_width, y + 3*(box_height + gap) + box_height/2),\n",
    "                 arrowprops=dict(arrowstyle=\"->\", lw=2, color='red', connectionstyle=\"arc3,rad=-0.3\"))\n",
    "        \n",
    "        # From attention to decoder\n",
    "        ax.annotate(\"\", xy=(x + box_width/2, y + 5*(box_height + gap)),\n",
    "                 xytext=(x + box_width/2, y + 7*(box_height + gap)),\n",
    "                 arrowprops=dict(arrowstyle=\"->\", lw=2, color='purple'))\n",
    "        \n",
    "        # Add parameter information\n",
    "        param_text = (\n",
    "            f\"Model Parameters:\\n\"\n",
    "            f\"  Encoder: {sum(p.numel() for p in model.encoder.parameters()):,}\\n\"\n",
    "            f\"  Decoder: {sum(p.numel() for p in model.decoder.parameters()):,}\\n\"\n",
    "            f\"    - Attention: {sum(p.numel() for p in model.decoder.attention.parameters()):,}\\n\"\n",
    "            f\"    - LSTM: {sum(p.numel() for p in model.decoder.lstm.parameters()):,}\\n\"\n",
    "            f\"  Total: {sum(p.numel() for p in model.parameters()):,}\\n\"\n",
    "            f\"  Trainable: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\"\n",
    "        )\n",
    "        \n",
    "        ax.text(0.02, 0.02, param_text, fontsize=10, ha='left', va='bottom',\n",
    "               bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "        \n",
    "        # Add legend for attention flow\n",
    "        legend_elements = [\n",
    "            patches.Patch(facecolor='blue', alpha=0.6, label='Image Features'),\n",
    "            patches.Patch(facecolor='red', alpha=0.6, label='Hidden State'),\n",
    "            patches.Patch(facecolor='purple', alpha=0.6, label='Attention Context')\n",
    "        ]\n",
    "        ax.legend(handles=legend_elements, loc='lower right', fontsize=10)\n",
    "        \n",
    "    else:\n",
    "        # =========== BASELINE MODEL VISUALIZATION ===========\n",
    "        y = 0.85\n",
    "        \n",
    "        # Encoder section\n",
    "        ax.text(0.5, 0.9, \"Encoder\", fontsize=14, fontweight='bold', ha='center')\n",
    "\n",
    "        # Input image\n",
    "        draw_box(x, y, box_width, box_height, \"Input Image\",\n",
    "                f\"Shape: (batch_size, 3, 224, 224)\", 'lightblue')\n",
    "        y -= box_height + gap\n",
    "\n",
    "        # ResNet\n",
    "        draw_box(x, y, box_width, box_height, \"ResNet-50 Backbone\",\n",
    "                \"Pretrained CNN, extracts visual features\", 'lightgreen')\n",
    "        y -= box_height + gap\n",
    "\n",
    "        # CNN Features\n",
    "        draw_box(x, y, box_width, box_height, \"CNN Features\",\n",
    "                f\"Shape: (batch_size, {model.encoder.feature_size}, 1, 1)\", 'lightyellow')\n",
    "        y -= box_height + gap\n",
    "\n",
    "        # Linear projection\n",
    "        draw_box(x, y, box_width, box_height, \"Linear Projection\",\n",
    "                f\"Shape: (batch_size, {model_config['embed_size']})\", 'lightgreen')\n",
    "        y -= box_height + gap\n",
    "\n",
    "        # Encoded Features\n",
    "        draw_box(x, y, box_width, box_height, \"Encoded Features\",\n",
    "                f\"Shape: (batch_size, {model_config['embed_size']})\", 'lightyellow')\n",
    "        y -= box_height + 2*gap\n",
    "\n",
    "        # Decoder section\n",
    "        ax.text(0.5, y, \"Decoder\", fontsize=14, fontweight='bold', ha='center')\n",
    "        y -= gap\n",
    "\n",
    "        # Caption Input\n",
    "        draw_box(x, y, box_width, box_height, \"Caption Input\",\n",
    "                \"Shape: (batch_size, seq_length)\", 'lightblue')\n",
    "        y -= box_height + gap\n",
    "\n",
    "        # Embedding\n",
    "        draw_box(x, y, box_width, box_height, \"Word Embedding\",\n",
    "                f\"Shape: (batch_size, seq_length, {model_config['embed_size']})\", 'lightgreen')\n",
    "        y -= box_height + gap\n",
    "\n",
    "        # LSTM\n",
    "        draw_box(x, y, box_width, box_height, \"LSTM\",\n",
    "                f\"Hidden size: {model_config['hidden_size']}, Layers: {model_config['num_layers']}\", 'lightgreen')\n",
    "        y -= box_height + gap\n",
    "\n",
    "        # Linear output\n",
    "        draw_box(x, y, box_width, box_height, \"Linear Output Layer\",\n",
    "                f\"Shape: (batch_size, seq_length, {len(vocab)})\", 'lightgreen')\n",
    "        y -= box_height + gap\n",
    "\n",
    "        # Final output\n",
    "        draw_box(x, y, box_width, box_height, \"Word Predictions\",\n",
    "                \"Cross-entropy loss during training\", 'lightyellow')\n",
    "\n",
    "        # Add arrows\n",
    "        for i in range(9):\n",
    "            y_pos = 0.85 - (i+0.5)*box_height - i*gap - (0.5*gap if i >= 5 else 0)\n",
    "            if i != 4:  # Skip arrow before decoder section\n",
    "                ax.annotate(\"\", xy=(x + box_width/2, y_pos - box_height - gap),\n",
    "                        xytext=(x + box_width/2, y_pos),\n",
    "                        arrowprops=dict(arrowstyle=\"->\", lw=2, color='black'))\n",
    "\n",
    "        # Add parameter information\n",
    "        param_text = (\n",
    "            f\"Model Parameters:\\n\"\n",
    "            f\"  Encoder: {sum(p.numel() for p in model.encoder.parameters()):,}\\n\"\n",
    "            f\"  Decoder: {sum(p.numel() for p in model.decoder.parameters()):,}\\n\"\n",
    "            f\"  Total: {sum(p.numel() for p in model.parameters()):,}\\n\"\n",
    "            f\"  Trainable: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\"\n",
    "        )\n",
    "\n",
    "        ax.text(0.02, 0.02, param_text, fontsize=10, ha='left', va='bottom',\n",
    "            bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "\n",
    "        # Add note about normalization constants\n",
    "        norm_text = (\n",
    "            f\"Image Normalization:\\n\"\n",
    "            f\"  Mean: {IMAGENET_MEAN}\\n\"\n",
    "            f\"  Std: {IMAGENET_STD}\\n\"\n",
    "            f\"  (ImageNet pretrained values)\"\n",
    "        )\n",
    "\n",
    "        ax.text(0.75, 0.02, norm_text, fontsize=9, ha='left', va='bottom',\n",
    "            bbox=dict(boxstyle='round', facecolor='lightyellow', alpha=0.8))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    save_path = os.path.join(LOGS_DIR, \"attention_architecture.png\" if is_attention_model else \"baseline_architecture.png\")\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9f50ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training Framework\n",
    "\n",
    "### 4.1 Loss and Metric Functions\n",
    "def calculate_loss(predictions, targets, criterion, pad_idx):\n",
    "    \"\"\"Calculate loss with attention to padding\"\"\"\n",
    "    # Create a mask to exclude padding tokens from loss\n",
    "    non_pad_mask = (targets != pad_idx)\n",
    "\n",
    "    # Count non-padding tokens\n",
    "    n_tokens = non_pad_mask.sum().item()\n",
    "\n",
    "    # Calculate loss\n",
    "    loss = criterion(predictions, targets)\n",
    "\n",
    "    return loss, n_tokens\n",
    "\n",
    "### 4.2 Unified Training Loop\n",
    "def train_epoch(model, data_loader, optimizer, criterion, clip, device, pad_idx, print_frequency=100):\n",
    "    \"\"\"Train model for one epoch - works for both baseline and attention models\"\"\"\n",
    "    \n",
    "    # Set model to training mode\n",
    "    model.train()\n",
    "    \n",
    "    # Initialize metrics\n",
    "    epoch_loss = 0\n",
    "    total_tokens = 0\n",
    "    batch_time = AverageMeter()  # Track batch processing time\n",
    "    \n",
    "    # Determine if this is an attention model\n",
    "    is_attention_model = isinstance(model, AttentionCaptionModel)\n",
    "    \n",
    "    # Get start time\n",
    "    start_time = time.time()\n",
    "    start_batch_time = time.time()\n",
    "    \n",
    "    # Iterate over batches with progress bar\n",
    "    with tqdm(total=len(data_loader), desc=\"Training\") as pbar:\n",
    "        try:\n",
    "            for i, (images, captions, lengths) in enumerate(data_loader):\n",
    "                # Move to device\n",
    "                images = images.to(device)\n",
    "                captions = captions.to(device)\n",
    "                \n",
    "                # Forward pass - different for baseline vs attention\n",
    "                if is_attention_model:\n",
    "                    outputs, alphas = model(images, captions, lengths)\n",
    "                    \n",
    "                    # Prepare targets (shift by one)\n",
    "                    targets = captions[:, 1:]  # Remove <SOS>\n",
    "                    outputs = outputs[:, :-1, :]  # Remove last prediction\n",
    "                    \n",
    "                    # Reshape for loss calculation\n",
    "                    batch_size = outputs.size(0)\n",
    "                    outputs = outputs.reshape(-1, outputs.size(2))\n",
    "                    targets = targets.reshape(-1)\n",
    "                    \n",
    "                    # Calculate loss\n",
    "                    loss, n_tokens = calculate_loss(outputs, targets, criterion, pad_idx)\n",
    "                    \n",
    "                    # Add attention regularization (encourage diversity in attention)\n",
    "                    alpha_c = 1.0  # Attention regularization factor\n",
    "                    att_reg = alpha_c * ((1 - alphas.sum(dim=1)) ** 2).mean()\n",
    "                    total_loss = loss + att_reg\n",
    "                else:\n",
    "                    outputs = model(images, captions, lengths)\n",
    "                    \n",
    "                    # Prepare targets (shift by one)\n",
    "                    targets = captions[:, 1:]  # Remove <SOS>\n",
    "                    outputs = outputs[:, :-1, :]  # Remove last prediction\n",
    "                    \n",
    "                    # Reshape for loss calculation\n",
    "                    batch_size = outputs.size(0)\n",
    "                    outputs = outputs.reshape(-1, outputs.size(2))\n",
    "                    targets = targets.reshape(-1)\n",
    "                    \n",
    "                    # Calculate loss\n",
    "                    loss, n_tokens = calculate_loss(outputs, targets, criterion, pad_idx)\n",
    "                    total_loss = loss\n",
    "                \n",
    "                # Update metrics\n",
    "                epoch_loss += total_loss.item() * n_tokens\n",
    "                total_tokens += n_tokens\n",
    "                \n",
    "                # Update batch time\n",
    "                batch_time.update(time.time() - start_batch_time)\n",
    "                start_batch_time = time.time()\n",
    "                \n",
    "                # Backward pass\n",
    "                optimizer.zero_grad()\n",
    "                total_loss.backward()\n",
    "                \n",
    "                # Clip gradients\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=clip)\n",
    "                \n",
    "                # Update weights\n",
    "                optimizer.step()\n",
    "                \n",
    "                # Update progress bar\n",
    "                pbar.update(1)\n",
    "                pbar.set_postfix({\"loss\": f\"{total_loss.item():.4f}\", \"time/batch\": f\"{batch_time.avg:.3f}s\"})\n",
    "                \n",
    "                # Print progress\n",
    "                if (i + 1) % print_frequency == 0:\n",
    "                    print(f\"Batch {i+1}/{len(data_loader)} | \"\n",
    "                         f\"Loss: {total_loss.item():.4f} | \"\n",
    "                         f\"Time: {batch_time.avg:.3f}s/batch | \"\n",
    "                         f\"Elapsed: {time.time() - start_time:.1f}s\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error during training: {str(e)}\")\n",
    "            traceback.print_exc()\n",
    "            raise\n",
    "    \n",
    "    # Calculate epoch metrics\n",
    "    avg_loss = epoch_loss / total_tokens if total_tokens > 0 else float('inf')\n",
    "    \n",
    "    return avg_loss\n",
    "\n",
    "### 4.3 Unified Validation Function\n",
    "def validate(model, data_loader, criterion, device, pad_idx):\n",
    "    \"\"\"Validate model - works for both baseline and attention models\"\"\"\n",
    "    \n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Determine if this is an attention model\n",
    "    is_attention_model = isinstance(model, AttentionCaptionModel)\n",
    "    \n",
    "    # Initialize metrics\n",
    "    epoch_loss = 0\n",
    "    total_tokens = 0\n",
    "    \n",
    "    # No gradient calculation needed\n",
    "    with torch.no_grad():\n",
    "        # Iterate over batches with progress bar\n",
    "        with tqdm(total=len(data_loader), desc=\"Validation\") as pbar:\n",
    "            for images, captions, lengths in data_loader:\n",
    "                # Move to device\n",
    "                images = images.to(device)\n",
    "                captions = captions.to(device)\n",
    "                \n",
    "                # Forward pass - different for baseline vs attention\n",
    "                if is_attention_model:\n",
    "                    outputs, alphas = model(images, captions, lengths)\n",
    "                    \n",
    "                    # Prepare targets (shift by one)\n",
    "                    targets = captions[:, 1:]  # Remove <SOS>\n",
    "                    outputs = outputs[:, :-1, :]  # Remove last prediction\n",
    "                    \n",
    "                    # Reshape for loss calculation\n",
    "                    outputs = outputs.reshape(-1, outputs.size(2))\n",
    "                    targets = targets.reshape(-1)\n",
    "                    \n",
    "                    # Calculate loss\n",
    "                    loss, n_tokens = calculate_loss(outputs, targets, criterion, pad_idx)\n",
    "                    \n",
    "                    # Add attention regularization\n",
    "                    alpha_c = 1.0  # Attention regularization factor\n",
    "                    att_reg = alpha_c * ((1 - alphas.sum(dim=1)) ** 2).mean()\n",
    "                    total_loss = loss + att_reg\n",
    "                else:\n",
    "                    outputs = model(images, captions, lengths)\n",
    "                    \n",
    "                    # Prepare targets (shift by one)\n",
    "                    targets = captions[:, 1:]  # Remove <SOS>\n",
    "                    outputs = outputs[:, :-1, :]  # Remove last prediction\n",
    "                    \n",
    "                    # Reshape for loss calculation\n",
    "                    outputs = outputs.reshape(-1, outputs.size(2))\n",
    "                    targets = targets.reshape(-1)\n",
    "                    \n",
    "                    # Calculate loss\n",
    "                    loss, n_tokens = calculate_loss(outputs, targets, criterion, pad_idx)\n",
    "                    total_loss = loss\n",
    "                \n",
    "                # Update metrics\n",
    "                epoch_loss += total_loss.item() * n_tokens\n",
    "                total_tokens += n_tokens\n",
    "                \n",
    "                # Update progress bar\n",
    "                pbar.update(1)\n",
    "                pbar.set_postfix({\"loss\": f\"{total_loss.item():.4f}\"})\n",
    "    \n",
    "    # Calculate epoch metrics\n",
    "    avg_loss = epoch_loss / total_tokens if total_tokens > 0 else float('inf')\n",
    "    \n",
    "    return avg_loss\n",
    "\n",
    "### 4.4 BLEU Score Calculation\n",
    "def calculate_bleu(model, data_loader, vocab, device, max_samples=None):\n",
    "    \"\"\"Calculate BLEU score - works with both model types\"\"\"\n",
    "    \n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Determine if this is an attention model\n",
    "    is_attention_model = isinstance(model, AttentionCaptionModel)\n",
    "    \n",
    "    # Initialize lists for references and hypotheses\n",
    "    references = []\n",
    "    hypotheses = []\n",
    "    \n",
    "    # No gradient calculation needed\n",
    "    with torch.no_grad():\n",
    "        # Progress bar\n",
    "        total = min(len(data_loader), max_samples // data_loader.batch_size + 1) if max_samples else len(data_loader)\n",
    "        with tqdm(total=total, desc=\"Calculating BLEU\") as pbar:\n",
    "            # Iterate over batches\n",
    "            for i, (images, captions, lengths) in enumerate(data_loader):\n",
    "                # Check if we've processed enough samples\n",
    "                if max_samples and i * data_loader.batch_size >= max_samples:\n",
    "                    break\n",
    "                \n",
    "                # Move to device\n",
    "                images = images.to(device)\n",
    "                \n",
    "                # Get predictions based on model type\n",
    "                if is_attention_model:\n",
    "                    predictions, _ = model.sample(images)\n",
    "                else:\n",
    "                    predictions = model.sample(images)\n",
    "                \n",
    "                # Process each image in the batch\n",
    "                for j in range(images.size(0)):\n",
    "                    # Check if we've processed enough samples\n",
    "                    if max_samples and len(hypotheses) >= max_samples:\n",
    "                        break\n",
    "                    \n",
    "                    # Get predicted caption\n",
    "                    pred_tokens = []\n",
    "                    for token_idx in predictions[j]:\n",
    "                        token = vocab.itos[token_idx.item()]\n",
    "                        if token == \"<EOS>\":\n",
    "                            break\n",
    "                        if token not in [\"<PAD>\", \"<SOS>\"]:\n",
    "                            pred_tokens.append(token)\n",
    "                    \n",
    "                    # Get reference caption\n",
    "                    ref_tokens = []\n",
    "                    for token_idx in captions[j]:\n",
    "                        token = vocab.itos[token_idx.item()]\n",
    "                        if token == \"<EOS>\":\n",
    "                            break\n",
    "                        if token not in [\"<PAD>\", \"<SOS>\"]:\n",
    "                            ref_tokens.append(token)\n",
    "                    \n",
    "                    # Add to lists\n",
    "                    references.append([ref_tokens])  # Each reference is a list of reference translations\n",
    "                    hypotheses.append(pred_tokens)\n",
    "                \n",
    "                # Update progress bar\n",
    "                pbar.update(1)\n",
    "    \n",
    "    # Calculate BLEU scores\n",
    "    bleu1 = corpus_bleu(references, hypotheses, weights=(1, 0, 0, 0))\n",
    "    bleu2 = corpus_bleu(references, hypotheses, weights=(0.5, 0.5, 0, 0))\n",
    "    bleu3 = corpus_bleu(references, hypotheses, weights=(0.33, 0.33, 0.33, 0))\n",
    "    bleu4 = corpus_bleu(references, hypotheses, weights=(0.25, 0.25, 0.25, 0.25))\n",
    "    \n",
    "    return {\n",
    "        \"bleu1\": bleu1 * 100,\n",
    "        \"bleu2\": bleu2 * 100,\n",
    "        \"bleu3\": bleu3 * 100,\n",
    "        \"bleu4\": bleu4 * 100\n",
    "    }\n",
    "\n",
    "### 4.5 Training Framework\n",
    "def train(model, train_loader, val_loader, optimizer, scheduler, criterion, config, device, vocab, model_paths):\n",
    "    \"\"\"Full training loop with both models\"\"\"\n",
    "    \n",
    "    # Unpack configuration parameters\n",
    "    num_epochs = config[\"num_epochs\"]\n",
    "    pad_idx = vocab.stoi[\"<PAD>\"]\n",
    "    print_frequency = config[\"print_frequency\"]\n",
    "    eval_every = config[\"eval_every\"]\n",
    "    bleu_every = config[\"bleu_every\"]\n",
    "    max_bleu_samples = config[\"max_bleu_samples\"]\n",
    "    early_stopping_patience = config[\"early_stopping_patience\"]\n",
    "    clip_grad_norm = config[\"clip_grad_norm\"]\n",
    "    \n",
    "    # Checkpoint paths\n",
    "    checkpoint_path = model_paths[\"checkpoint_path\"]\n",
    "    best_model_path = model_paths[\"best_model_path\"]\n",
    "    \n",
    "    # Determine model type\n",
    "    is_attention_model = isinstance(model, AttentionCaptionModel)\n",
    "    model_name = \"Attention\" if is_attention_model else \"Baseline\"\n",
    "    \n",
    "    # Initialize tracking variables\n",
    "    best_val_loss = float('inf')\n",
    "    best_bleu = 0\n",
    "    patience_counter = 0\n",
    "    history = {\n",
    "        'epochs': [],\n",
    "        'train_losses': [],\n",
    "        'val_epochs': [],\n",
    "        'val_losses': [],\n",
    "        'bleu_epochs': [],\n",
    "        'bleu_scores': []\n",
    "    }\n",
    "    \n",
    "    # Check if a trained model exists\n",
    "    model_status, model_checkpoint = check_model_availability(config, best_model_path, checkpoint_path)\n",
    "    \n",
    "    # If a trained model exists, load it\n",
    "    if model_status == \"trained\":\n",
    "        print(f\"Found trained {model_name} model, loading weights...\")\n",
    "        model.load_state_dict(model_checkpoint['state_dict'])\n",
    "        \n",
    "        # Load training history if available\n",
    "        if 'training_history' in model_checkpoint:\n",
    "            history = model_checkpoint['training_history']\n",
    "            print(\"Loaded training history.\")\n",
    "        \n",
    "        print(f\"Skipping training for {model_name} model.\")\n",
    "        return model, history\n",
    "    \n",
    "    # If a checkpoint exists, resume training\n",
    "    elif model_status == \"checkpoint\":\n",
    "        print(f\"Found checkpoint for {model_name} model, resuming training...\")\n",
    "        \n",
    "        # Load model state\n",
    "        model.load_state_dict(model_checkpoint['state_dict'])\n",
    "        \n",
    "        # Load optimizer and scheduler states if available\n",
    "        if 'optimizer' in model_checkpoint and optimizer is not None:\n",
    "            optimizer.load_state_dict(model_checkpoint['optimizer'])\n",
    "            print(\"Loaded optimizer state.\")\n",
    "        \n",
    "        if 'scheduler' in model_checkpoint and scheduler is not None:\n",
    "            scheduler.load_state_dict(model_checkpoint['scheduler'])\n",
    "            print(\"Loaded scheduler state.\")\n",
    "        \n",
    "        # Load training history if available\n",
    "        if 'training_history' in model_checkpoint:\n",
    "            history = model_checkpoint['training_history']\n",
    "            print(\"Loaded training history.\")\n",
    "        \n",
    "        # Get starting epoch\n",
    "        start_epoch = model_checkpoint.get('epoch', 0)\n",
    "        best_val_loss = model_checkpoint.get('val_loss', float('inf'))\n",
    "        print(f\"Resuming training from epoch {start_epoch + 1}...\")\n",
    "        \n",
    "    else:\n",
    "        # Start fresh training\n",
    "        print(f\"Starting fresh training for {model_name} model...\")\n",
    "        start_epoch = 0\n",
    "    \n",
    "    # Start training time\n",
    "    train_start_time = time.time()\n",
    "    \n",
    "    print(f\"\\nTraining {model_name} model for {num_epochs - start_epoch} epochs...\")\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        epoch_num = epoch + 1  # 1-based epoch numbering\n",
    "        \n",
    "        # Start epoch time\n",
    "        epoch_start_time = time.time()\n",
    "        \n",
    "        # Print epoch info\n",
    "        print(f\"\\nEpoch {epoch_num}/{num_epochs}\")\n",
    "        \n",
    "        # Train for one epoch\n",
    "        train_loss = train_epoch(\n",
    "            model, train_loader, optimizer, criterion,\n",
    "            clip_grad_norm, device, pad_idx, print_frequency\n",
    "        )\n",
    "        \n",
    "        # Store train loss\n",
    "        history['epochs'].append(epoch_num)\n",
    "        history['train_losses'].append(train_loss)\n",
    "        \n",
    "        # Validation (based on eval_every)\n",
    "        should_validate = epoch_num % eval_every == 0 or epoch_num == num_epochs\n",
    "        if should_validate:\n",
    "            # Validate\n",
    "            val_loss = validate(model, val_loader, criterion, device, pad_idx)\n",
    "            \n",
    "            # Store validation results\n",
    "            history['val_epochs'].append(epoch_num)\n",
    "            history['val_losses'].append(val_loss)\n",
    "            \n",
    "            # Check for new best model\n",
    "            is_best = val_loss < best_val_loss\n",
    "            if is_best:\n",
    "                best_val_loss = val_loss\n",
    "                patience_counter = 0\n",
    "                \n",
    "                # Save best model\n",
    "                print(f\"New best model with validation loss: {val_loss:.4f}\")\n",
    "                save_checkpoint({\n",
    "                    'epoch': epoch_num,\n",
    "                    'state_dict': model.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'scheduler': scheduler.state_dict() if scheduler else None,\n",
    "                    'val_loss': val_loss,\n",
    "                    'config': config,\n",
    "                    'training_history': history\n",
    "                }, is_best=True, filepath=best_model_path)\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "            \n",
    "            # Print validation results\n",
    "            print(f\"Epoch {epoch_num} - Train loss: {train_loss:.4f}, Val loss: {val_loss:.4f}\")\n",
    "            \n",
    "            # Update learning rate scheduler\n",
    "            if scheduler:\n",
    "                scheduler.step(val_loss)\n",
    "        else:\n",
    "            # Print training results only\n",
    "            print(f\"Epoch {epoch_num} - Train loss: {train_loss:.4f}\")\n",
    "        \n",
    "        # Calculate BLEU scores (based on bleu_every)\n",
    "        should_calculate_bleu = epoch_num % bleu_every == 0 or epoch_num == num_epochs\n",
    "        if should_calculate_bleu:\n",
    "            # Calculate BLEU scores\n",
    "            print(\"Calculating BLEU scores...\")\n",
    "            bleu = calculate_bleu(model, val_loader, vocab, device, max_samples=max_bleu_samples)\n",
    "            \n",
    "            # Store BLEU scores\n",
    "            history['bleu_epochs'].append(epoch_num)\n",
    "            history['bleu_scores'].append(bleu)\n",
    "            \n",
    "            # Check for new best model (based on BLEU-4)\n",
    "            if bleu['bleu4'] > best_bleu:\n",
    "                best_bleu = bleu['bleu4']\n",
    "            \n",
    "            # Print BLEU scores\n",
    "            print(f\"BLEU scores - BLEU-1: {bleu['bleu1']:.2f}, BLEU-2: {bleu['bleu2']:.2f}, \"\n",
    "                  f\"BLEU-3: {bleu['bleu3']:.2f}, BLEU-4: {bleu['bleu4']:.2f}\")\n",
    "        \n",
    "        # Save regular checkpoint\n",
    "        save_checkpoint({\n",
    "            'epoch': epoch_num,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'scheduler': scheduler.state_dict() if scheduler else None,\n",
    "            'train_loss': train_loss,\n",
    "            'val_loss': val_loss if should_validate else None,\n",
    "            'config': config,\n",
    "            'training_history': history\n",
    "        }, filepath=checkpoint_path)\n",
    "        \n",
    "        # Check for early stopping\n",
    "        if patience_counter >= early_stopping_patience:\n",
    "            print(f\"Early stopping triggered after {epoch_num} epochs\")\n",
    "            break\n",
    "        \n",
    "        # Print epoch time\n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        print(f\"Epoch {epoch_num} completed in {epoch_time:.1f}s\")\n",
    "    \n",
    "    # Print total training time\n",
    "    train_time = time.time() - train_start_time\n",
    "    print(f\"\\nTraining completed in {train_time/60:.1f} minutes\")\n",
    "    \n",
    "    # Load the best model\n",
    "    try:\n",
    "        print(f\"Loading best {model_name} model...\")\n",
    "        best_checkpoint = torch.load(best_model_path)\n",
    "        model.load_state_dict(best_checkpoint['state_dict'])\n",
    "        print(f\"Loaded best model from epoch {best_checkpoint['epoch']} with validation loss {best_checkpoint['val_loss']:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not load best model: {e}\")\n",
    "        print(\"Using final model instead.\")\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "### 4.6 Visualization Functions\n",
    "def plot_training_history(history, model_name=\"Model\"):\n",
    "    \"\"\"Plot training history with proper handling of evaluation frequencies\"\"\"\n",
    "\n",
    "    # Create a figure\n",
    "    plt.figure(figsize=(14, 6))\n",
    "\n",
    "    # Plot loss curves\n",
    "    plt.subplot(1, 2, 1)\n",
    "\n",
    "    # Plot training loss (always available)\n",
    "    plt.plot(history['epochs'], history['train_losses'], 'o-', label='Train Loss')\n",
    "\n",
    "    # Plot validation loss if available\n",
    "    if 'val_epochs' in history and history['val_losses']:\n",
    "        plt.plot(history['val_epochs'], history['val_losses'], 'o-', label='Val Loss')\n",
    "\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(f'{model_name} - Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Plot BLEU scores if available\n",
    "    if 'bleu_epochs' in history and history['bleu_scores']:\n",
    "        plt.subplot(1, 2, 2)\n",
    "\n",
    "        # Extract BLEU scores\n",
    "        bleu_epochs = history['bleu_epochs']\n",
    "        bleu1 = [b['bleu1'] for b in history['bleu_scores']]\n",
    "        bleu2 = [b['bleu2'] for b in history['bleu_scores']]\n",
    "        bleu3 = [b['bleu3'] for b in history['bleu_scores']]\n",
    "        bleu4 = [b['bleu4'] for b in history['bleu_scores']]\n",
    "\n",
    "        # Plot BLEU scores\n",
    "        plt.plot(bleu_epochs, bleu1, 'o-', label='BLEU-1')\n",
    "        plt.plot(bleu_epochs, bleu2, 'o-', label='BLEU-2')\n",
    "        plt.plot(bleu_epochs, bleu3, 'o-', label='BLEU-3')\n",
    "        plt.plot(bleu_epochs, bleu4, 'o-', label='BLEU-4')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('BLEU Score')\n",
    "        plt.title(f'{model_name} - BLEU Scores')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "\n",
    "    # Show the figure\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save the figure\n",
    "    plt.savefig(os.path.join(LOGS_DIR, f'{model_name.lower()}_training_history.png'), dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "def generate_caption(model, image, vocab, max_length=20):\n",
    "    \"\"\"Generate caption for image - works with both models\"\"\"\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    # Check model type and use appropriate method\n",
    "    if isinstance(model, AttentionCaptionModel):\n",
    "        caption, attention_weights = model.caption_image_with_attention(image, vocab, max_length)\n",
    "        return caption, attention_weights\n",
    "    else:\n",
    "        caption = model.caption_image(image, vocab, max_length)\n",
    "        return caption, None\n",
    "\n",
    "def visualize_sample_captions(model, dataset, vocab, device, num_samples=3):\n",
    "    \"\"\"Visualize sample captions generated by the model\"\"\"\n",
    "    \n",
    "    # Determine if this is an attention model\n",
    "    is_attention_model = isinstance(model, AttentionCaptionModel)\n",
    "\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Select random samples\n",
    "    indices = np.random.choice(len(dataset), num_samples, replace=False)\n",
    "\n",
    "    # Generate captions for each sample\n",
    "    with torch.no_grad():\n",
    "        for i, idx in enumerate(indices):\n",
    "            # Get image and caption\n",
    "            image, caption = dataset[idx]\n",
    "            img_name = dataset.data_df.iloc[idx]['image']  # Get image name\n",
    "\n",
    "            # Move to device\n",
    "            image = image.unsqueeze(0).to(device)\n",
    "\n",
    "            # Generate caption\n",
    "            if is_attention_model:\n",
    "                generated_caption, attention_weights = model.caption_image_with_attention(image, vocab)\n",
    "            else:\n",
    "                generated_caption = model.caption_image(image, vocab)\n",
    "                attention_weights = None\n",
    "\n",
    "            # Convert reference caption to words\n",
    "            reference_caption = []\n",
    "            for token_idx in caption:\n",
    "                token = vocab.itos[token_idx.item()]\n",
    "                if token == \"<EOS>\":\n",
    "                    break\n",
    "                if token not in [\"<PAD>\", \"<SOS>\"]:\n",
    "                    reference_caption.append(token)\n",
    "            reference_caption = ' '.join(reference_caption)\n",
    "\n",
    "            # Print image name and captions\n",
    "            print()\n",
    "            print(f\"Image {i+1}: {img_name}\")\n",
    "            print(f\"Reference: {reference_caption}\")\n",
    "            print(f\"Generated: {generated_caption}\")\n",
    "            \n",
    "            # Display the image\n",
    "            plt.figure(figsize=(10, 8))\n",
    "            img = denormalize_image(image[0])\n",
    "            plt.imshow(img)\n",
    "            plt.title(f\"Image: {img_name}\")\n",
    "            plt.axis('off')\n",
    "            plt.tight_layout()\n",
    "            save_path = os.path.join(LOGS_DIR, f'caption_sample_{i+1}.png')\n",
    "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "            plt.show()\n",
    "            \n",
    "            # For attention model, visualize attention weights\n",
    "            if is_attention_model and attention_weights:\n",
    "                visualize_attention(img, generated_caption.split(), attention_weights)\n",
    "\n",
    "def visualize_attention(image, caption, attention_weights, show_every=1, save_path=None):\n",
    "    \"\"\"Visualize attention for each word in the caption\"\"\"\n",
    "    # Create figure with subplots - one subplot per word, plus one for the original image\n",
    "    words_to_show = list(range(0, len(caption), show_every))\n",
    "    num_words = len(words_to_show)\n",
    "    \n",
    "    # Determine subplot grid size\n",
    "    if num_words < 6:\n",
    "        # For few words, use 1 row\n",
    "        nrows = 1\n",
    "        ncols = num_words + 1  # +1 for original image\n",
    "    else:\n",
    "        # For more words, use 2 or more rows\n",
    "        ncols = min(5, num_words + 1)\n",
    "        nrows = (num_words + 1 + ncols - 1) // ncols\n",
    "    \n",
    "    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(ncols*3, nrows*3))\n",
    "    \n",
    "    # Handle different subplot layouts correctly\n",
    "    if nrows == 1 and ncols == 1:\n",
    "        # Single subplot\n",
    "        axes_flat = [axes]\n",
    "    elif nrows == 1 or ncols == 1:\n",
    "        # 1D array of subplots (single row or column)\n",
    "        axes_flat = axes.flatten()\n",
    "    else:\n",
    "        # 2D array of subplots\n",
    "        axes_flat = axes.flatten()\n",
    "    \n",
    "    # Plot original image\n",
    "    axes_flat[0].imshow(image)\n",
    "    axes_flat[0].set_title('Original Image')\n",
    "    axes_flat[0].axis('off')\n",
    "    \n",
    "    # Plot attention for each selected word\n",
    "    for idx, word_idx in enumerate(words_to_show):\n",
    "        if idx + 1 >= len(axes_flat):  # Ensure we don't run out of subplots\n",
    "            break\n",
    "            \n",
    "        # Get attention weights and word\n",
    "        att_weight = attention_weights[word_idx]\n",
    "        word = caption[word_idx]\n",
    "        \n",
    "        # Reshape attention weights to square for visualization\n",
    "        # Assuming it's a square feature map\n",
    "        att_size = int(np.sqrt(att_weight.shape[0]))\n",
    "        att_weight = att_weight.reshape(att_size, att_size)\n",
    "        \n",
    "        # Resize attention map to match image size\n",
    "        h, w = image.shape[:2]\n",
    "        att_weight = np.repeat(np.repeat(att_weight, h//att_size, axis=0), w//att_size, axis=1)\n",
    "        att_weight = att_weight[:h, :w]  # Crop to match image size\n",
    "        \n",
    "        # Plot the word-specific attention\n",
    "        axes_flat[idx + 1].imshow(image)\n",
    "        axes_flat[idx + 1].imshow(att_weight, alpha=0.6, cmap='hot')\n",
    "        axes_flat[idx + 1].set_title(word)\n",
    "        axes_flat[idx + 1].axis('off')\n",
    "    \n",
    "    # Hide any unused subplots\n",
    "    for i in range(len(words_to_show) + 1, len(axes_flat)):\n",
    "        axes_flat[i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, bbox_inches='tight')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "def compare_models(baseline_model, attention_model, dataset, vocab, device, num_samples=3):\n",
    "    \"\"\"Compare captions generated by baseline and attention models\"\"\"\n",
    "    \n",
    "    # Set models to evaluation mode\n",
    "    baseline_model.eval()\n",
    "    attention_model.eval()\n",
    "    \n",
    "    # Select random samples\n",
    "    indices = np.random.choice(len(dataset), num_samples, replace=False)\n",
    "    \n",
    "    # Generate captions for each sample\n",
    "    with torch.no_grad():\n",
    "        for i, idx in enumerate(indices):\n",
    "            # Get image and ground truth caption\n",
    "            image, caption = dataset[idx]\n",
    "            img_name = dataset.data_df.iloc[idx]['image']\n",
    "            \n",
    "            # Move to device\n",
    "            image = image.unsqueeze(0).to(device)\n",
    "            \n",
    "            # Generate captions\n",
    "            baseline_caption = baseline_model.caption_image(image, vocab)\n",
    "            attention_caption, attention_weights = attention_model.caption_image_with_attention(image, vocab)\n",
    "            \n",
    "            # Convert ground truth caption to words\n",
    "            gt_words = [vocab.itos[idx.item()] for idx in caption\n",
    "                      if idx.item() < len(vocab) and vocab.itos[idx.item()] not in [\"<PAD>\", \"<SOS>\", \"<EOS>\"]]\n",
    "            gt_caption = ' '.join(gt_words)\n",
    "            \n",
    "            # Print image name and captions\n",
    "            print(f\"\\nImage {i+1}: {img_name}\")\n",
    "            print(f\"Ground Truth: {gt_caption}\")\n",
    "            print(f\"Baseline: {baseline_caption}\")\n",
    "            print(f\"Attention: {attention_caption}\")\n",
    "            \n",
    "            # Display image and captions\n",
    "            plt.figure(figsize=(12, 10))\n",
    "            \n",
    "            # Display image\n",
    "            plt.subplot(2, 1, 1)\n",
    "            img = denormalize_image(image[0])\n",
    "            plt.imshow(img)\n",
    "            plt.title(f\"Image: {img_name}\")\n",
    "            plt.axis('off')\n",
    "            \n",
    "            # Display captions comparison\n",
    "            plt.subplot(2, 1, 2)\n",
    "            plt.axis('off')\n",
    "            comparison_text = (\n",
    "                f\"Ground Truth: {gt_caption}\\n\\n\"\n",
    "                f\"Baseline: {baseline_caption}\\n\\n\"\n",
    "                f\"Attention: {attention_caption}\"\n",
    "            )\n",
    "            plt.text(0.5, 0.5, comparison_text, ha='center', va='center', fontsize=12, wrap=True)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            save_path = os.path.join(LOGS_DIR, f'model_comparison_{i+1}.png')\n",
    "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "            plt.show()\n",
    "            \n",
    "            # Display attention visualization\n",
    "            attention_words = attention_caption.split()\n",
    "            if attention_weights and attention_words:\n",
    "                # Limit number of words to display for readability\n",
    "                show_every = max(1, len(attention_words) // 10)\n",
    "                visualize_attention(\n",
    "                    img, attention_words, attention_weights,\n",
    "                    show_every=show_every,\n",
    "                    save_path=os.path.join(LOGS_DIR, f'model_comparison_attention_{i+1}.png')\n",
    "                )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8f03c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model Training and Evaluation\n",
    "\n",
    "### 5.1 Initialize and Train Baseline Model\n",
    "# Initialize baseline model\n",
    "baseline_model = BaselineCaptionModel(\n",
    "    embed_size=model_config[\"embed_size\"],\n",
    "    hidden_size=model_config[\"hidden_size\"],\n",
    "    vocab_size=len(vocab),\n",
    "    num_layers=model_config[\"num_layers\"],\n",
    "    dropout=model_config[\"dropout\"]\n",
    ").to(device)\n",
    "\n",
    "# Create loss criterion\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=vocab.stoi[\"<PAD>\"])\n",
    "\n",
    "# Create optimizer for baseline model\n",
    "baseline_optimizer = optim.Adam(\n",
    "    baseline_model.parameters(),\n",
    "    lr=model_config[\"learning_rate\"],\n",
    "    weight_decay=model_config[\"weight_decay\"]\n",
    ")\n",
    "\n",
    "# Create learning rate scheduler\n",
    "baseline_scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    baseline_optimizer,\n",
    "    mode='min',\n",
    "    factor=model_config[\"lr_scheduler_factor\"],\n",
    "    patience=model_config[\"lr_scheduler_patience\"],\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Visualize baseline model architecture\n",
    "visualize_model_architecture(baseline_model, is_attention_model=False)\n",
    "\n",
    "# Train baseline model\n",
    "print(\"\\nTraining baseline model...\")\n",
    "baseline_model, baseline_history = train(\n",
    "    baseline_model, train_loader_to_use, val_loader_to_use,\n",
    "    baseline_optimizer, baseline_scheduler, criterion,\n",
    "    model_config, device, vocab, baseline_paths\n",
    ")\n",
    "\n",
    "# Plot baseline training history\n",
    "plot_training_history(baseline_history, model_name=\"Baseline\")\n",
    "\n",
    "# Visualize sample captions from baseline model\n",
    "print(\"\\nGenerating sample captions from baseline model...\")\n",
    "visualize_sample_captions(baseline_model, test_dataset, vocab, device, num_samples=3)\n",
    "\n",
    "### 5.2 Initialize and Train Attention Model\n",
    "# Initialize attention model\n",
    "attention_model = AttentionCaptionModel(\n",
    "    embed_size=model_config[\"embed_size\"],\n",
    "    hidden_size=model_config[\"hidden_size\"],\n",
    "    vocab_size=len(vocab),\n",
    "    attention_dim=model_config[\"attention_dim\"],\n",
    "    num_layers=model_config[\"num_layers\"],\n",
    "    dropout=model_config[\"dropout\"]\n",
    ").to(device)\n",
    "\n",
    "# Create optimizer for attention model\n",
    "attention_optimizer = optim.Adam(\n",
    "    attention_model.parameters(),\n",
    "    lr=model_config[\"learning_rate\"],\n",
    "    weight_decay=model_config[\"weight_decay\"]\n",
    ")\n",
    "\n",
    "# Create learning rate scheduler\n",
    "attention_scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    attention_optimizer,\n",
    "    mode='min',\n",
    "    factor=model_config[\"lr_scheduler_factor\"],\n",
    "    patience=model_config[\"lr_scheduler_patience\"],\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Visualize attention model architecture\n",
    "visualize_model_architecture(attention_model, is_attention_model=True)\n",
    "\n",
    "# Train attention model\n",
    "print(\"\\nTraining attention model...\")\n",
    "attention_model, attention_history = train(\n",
    "    attention_model, train_loader_to_use, val_loader_to_use,\n",
    "    attention_optimizer, attention_scheduler, criterion,\n",
    "    model_config, device, vocab, attention_paths\n",
    ")\n",
    "\n",
    "# Plot attention training history\n",
    "plot_training_history(attention_history, model_name=\"Attention\")\n",
    "\n",
    "# Visualize sample captions from attention model\n",
    "print(\"\\nGenerating sample captions from attention model...\")\n",
    "visualize_sample_captions(attention_model, test_dataset, vocab, device, num_samples=3)\n",
    "\n",
    "### 5.3 Evaluate and Compare Models\n",
    "# Calculate BLEU scores for both models\n",
    "print(\"\\nCalculating final BLEU scores for baseline model...\")\n",
    "baseline_bleu = calculate_bleu(\n",
    "    baseline_model, test_loader_to_use, vocab, device,\n",
    "    max_samples=model_config[\"max_bleu_samples\"]\n",
    ")\n",
    "\n",
    "print(\"\\nCalculating final BLEU scores for attention model...\")\n",
    "attention_bleu = calculate_bleu(\n",
    "    attention_model, test_loader_to_use, vocab, device,\n",
    "    max_samples=model_config[\"max_bleu_samples\"]\n",
    ")\n",
    "\n",
    "# Print BLEU comparison\n",
    "print(\"\\nModel BLEU Score Comparison:\")\n",
    "print(f\"                 BLEU-1  BLEU-2  BLEU-3  BLEU-4\")\n",
    "print(f\"Baseline Model:  {baseline_bleu['bleu1']:.2f}    {baseline_bleu['bleu2']:.2f}    {baseline_bleu['bleu3']:.2f}    {baseline_bleu['bleu4']:.2f}\")\n",
    "print(f\"Attention Model: {attention_bleu['bleu1']:.2f}    {attention_bleu['bleu2']:.2f}    {attention_bleu['bleu3']:.2f}    {attention_bleu['bleu4']:.2f}\")\n",
    "\n",
    "# Compare caption quality and attention visualization\n",
    "print(\"\\nComparing caption quality between models...\")\n",
    "compare_models(baseline_model, attention_model, test_dataset, vocab, device, num_samples=3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53a197d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Summary and Conclusion\n",
    "print(\"\\nProject Summary:\")\n",
    "print(\"1. Implemented both baseline CNN+RNN and attention-based image captioning models\")\n",
    "print(\"2. The attention mechanism helps the model focus on relevant image regions for each word\")\n",
    "print(\"3. Attention model generally produces more detailed and accurate captions\")\n",
    "print(\"4. Attention visualizations provide interpretable insights into model behavior\")\n",
    "\n",
    "print(\"\\nKey Differences:\")\n",
    "print(\"1. Encoder: Attention model preserves spatial information, baseline pools features\")\n",
    "print(\"2. Decoder: Attention model uses weighted context vectors, baseline uses global features\")\n",
    "print(\"3. Training: Attention model includes attention regularization term in loss function\")\n",
    "print(\"4. Inference: Attention model dynamically focuses on different regions for each word\")\n",
    "\n",
    "print(\"\\nFuture Improvements:\")\n",
    "print(\"1. Use a more powerful CNN backbone like EfficientNet or Vision Transformer\")\n",
    "print(\"2. Implement beam search instead of greedy decoding for better captions\")\n",
    "print(\"3. Train on larger datasets like MS COCO for better generalization\")\n",
    "print(\"4. Incorporate semantic understanding with pre-trained language models\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
